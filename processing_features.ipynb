{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing Demo\n",
    "\n",
    "This notebook demonstrates the document processing pipeline used in the Content Verification Tool:\n",
    "\n",
    "1. **Docling PDF Conversion** - Converting PDF to DoclingDocument\n",
    "2. **Docling DOCX Conversion** - Converting DOCX to PDF then DoclingDocument (with LibreOffice)\n",
    "3. **Hierarchical Pre-chunking** - Using HybridChunker from Docling\n",
    "4. **Paragraph-level Splitting** - Using LangChain RecursiveCharacterTextSplitter\n",
    "5. **Sentence-level Splitting** - Using SpaCy sentence detection\n",
    "6. **Verification Shell Creation** - Creating DocumentChunk objects with page # and item # assignments\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install dependencies (already in pyproject.toml)\n",
    "pip install docling docling-core langchain-text-splitters spacy python-docx termcolor\n",
    "\n",
    "# Download SpaCy model\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "## Sample Documents\n",
    "\n",
    "This demo requires sample PDF and DOCX files. You can:\n",
    "- Use your own legal documents (max 10MB)\n",
    "- Create sample files named `sample.pdf` and `sample.docx` in the project root\n",
    "- Update the file paths in the code cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Initialize Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[CACHE] Initialized cache directory: /tmp/document_cache\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurentwiesel/Dev/ai-law/content_verification_tool/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m[PROCESSOR] Initializing Docling DocumentConverter...\u001b[0m\n",
      "\u001b[32m[PROCESSOR] LibreOffice found at: /Applications/LibreOffice.app/Contents/MacOS/soffice\u001b[0m\n",
      "\u001b[32m[PROCESSOR] DocumentConverter initialized successfully\u001b[0m\n",
      "\u001b[36m[CHUNKER] Initializing chunking strategies...\u001b[0m\n",
      "\u001b[32m[CHUNKER] Chunking strategies initialized\u001b[0m\n",
      "\u001b[36m[OUTPUT] Initialized output directory: /tmp/output\u001b[0m\n",
      "================================================================================\n",
      "INITIALIZING DOCUMENT PROCESSING PIPELINE\n",
      "================================================================================\n",
      "\u001b[36m[PROCESSOR] Initializing Docling DocumentConverter...\u001b[0m\n",
      "\u001b[32m[PROCESSOR] LibreOffice found at: /Applications/LibreOffice.app/Contents/MacOS/soffice\u001b[0m\n",
      "\u001b[32m[PROCESSOR] DocumentConverter initialized successfully\u001b[0m\n",
      "\u001b[36m[CHUNKER] Initializing chunking strategies...\u001b[0m\n",
      "\u001b[32m[CHUNKER] Chunking strategies initialized\u001b[0m\n",
      "\n",
      "‚úì All components initialized successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# Add backend to path for imports\n",
    "sys.path.insert(0, str(Path.cwd() / 'backend'))\n",
    "\n",
    "from app.processing.document_processor import DocumentProcessor\n",
    "from app.processing.chunker import DocumentChunker\n",
    "from app.models import ChunkingMode, DocumentChunk\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INITIALIZING DOCUMENT PROCESSING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize processor and chunker\n",
    "processor = DocumentProcessor()\n",
    "chunker = DocumentChunker()\n",
    "\n",
    "print(\"\\n‚úì All components initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: PDF Processing with Docling\n",
    "\n",
    "We'll convert a PDF document to a DoclingDocument object, which preserves:\n",
    "- Page structure and numbers\n",
    "- Paragraphs and sections\n",
    "- Tables\n",
    "- Footnotes\n",
    "- Provenance data (for tracking text spans across pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:58:57,639 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 1: PDF ‚Üí DoclingDocument\n",
      "================================================================================\n",
      "\n",
      "üìÑ Processing PDF: AgentQuality-Abridged.pdf\n",
      "   File size: 134.63 KB\n",
      "\u001b[36m[PROCESSOR] Converting document: AgentQuality-Abridged.pdf\u001b[0m\n",
      "\u001b[32m[PROCESSOR] File validation passed: AgentQuality-Abridged.pdf (134.63 KB)\u001b[0m\n",
      "\u001b[33m[CACHE] Cache MISS for document d53ac937...\u001b[0m\n",
      "\u001b[36m[PROCESSOR] Native PDF detected, processing with OCR disabled\u001b[0m\n",
      "\u001b[36m[PROCESSOR] Running Docling conversion on tmpbz4z3_pi.pdf...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:58:57,695 - INFO - Going to convert document batch...\n",
      "2025-11-16 12:58:57,702 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 70256a236a6856c82de2c96fe229a58e\n",
      "2025-11-16 12:58:57,730 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-16 12:58:57,734 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-11-16 12:58:57,741 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-11-16 12:58:57,748 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-11-16 12:58:57,792 - INFO - Accelerator device: 'mps'\n",
      "2025-11-16 12:59:00,046 - INFO - Accelerator device: 'mps'\n",
      "2025-11-16 12:59:00,811 - INFO - Processing document tmpbz4z3_pi.pdf\n",
      "2025-11-16 12:59:23,206 - INFO - Finished converting document tmpbz4z3_pi.pdf in 25.57 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[PROCESSOR] Conversion successful: 4 pages\u001b[0m\n",
      "\u001b[32m[CACHE] Cached document d53ac937...\u001b[0m\n",
      "\u001b[36m[PROCESSOR] Cleaned up temporary DOCX file\u001b[0m\n",
      "\n",
      "‚úì Conversion successful!\n",
      "   Filename: AgentQuality-Abridged.pdf\n",
      "   Pages: 4\n",
      "   File size: 137865 bytes\n",
      "\n",
      "üìä DoclingDocument Structure:\n",
      "   Type: <class 'docling_core.types.doc.document.DoclingDocument'>\n",
      "   Has pages: True\n",
      "   Total pages: 4\n",
      "   Page keys (first 5): [1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Define path to sample PDF\n",
    "PDF_PATH = \"AgentQuality-Abridged.pdf\"  # User should replace with their file\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1: PDF ‚Üí DoclingDocument\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if file exists\n",
    "if not Path(PDF_PATH).exists():\n",
    "    print(f\"\\n‚ùå ERROR: File not found: {PDF_PATH}\")\n",
    "    print(\"Please create a sample PDF file or update PDF_PATH variable\")\n",
    "else:\n",
    "    # Read file content\n",
    "    with open(PDF_PATH, \"rb\") as f:\n",
    "        pdf_content = f.read()\n",
    "    \n",
    "    print(f\"\\nüìÑ Processing PDF: {PDF_PATH}\")\n",
    "    print(f\"   File size: {len(pdf_content) / 1024:.2f} KB\")\n",
    "    \n",
    "    # Convert with Docling\n",
    "    result = processor.convert_document(pdf_content, PDF_PATH, use_cache=True)\n",
    "    \n",
    "    docling_doc = result['docling_document']\n",
    "    \n",
    "    print(f\"\\n‚úì Conversion successful!\")\n",
    "    print(f\"   Filename: {result['filename']}\")\n",
    "    print(f\"   Pages: {result['page_count']}\")\n",
    "    print(f\"   File size: {result['file_size']} bytes\")\n",
    "    \n",
    "    # Inspect DoclingDocument structure\n",
    "    print(f\"\\nüìä DoclingDocument Structure:\")\n",
    "    print(f\"   Type: {type(docling_doc)}\")\n",
    "    print(f\"   Has pages: {hasattr(docling_doc, 'pages')}\")\n",
    "    if hasattr(docling_doc, 'pages') and docling_doc.pages:\n",
    "        print(f\"   Total pages: {len(docling_doc.pages)}\")\n",
    "        # Pages is a dict, not a list - show first few keys\n",
    "        page_keys = list(docling_doc.pages.keys())\n",
    "        print(f\"   Page keys (first 5): {page_keys[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DOCX Processing with LibreOffice + Docling\n",
    "\n",
    "DOCX files are converted to PDF using LibreOffice first, then processed with Docling.\n",
    "\n",
    "**Why?**\n",
    "- Accurate page numbers (DOCX doesn't have fixed pages)\n",
    "- Consistent processing pipeline (everything goes through PDF)\n",
    "- Better metadata extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 2: DOCX ‚Üí PDF ‚Üí DoclingDocument\n",
      "================================================================================\n",
      "\n",
      "üìÑ Processing DOCX: AgentQuality-ShortSummary.docx\n",
      "   File size: 14.86 KB\n",
      "\n",
      "‚ö†Ô∏è  Note: This will use LibreOffice for DOCX‚ÜíPDF conversion\n",
      "\u001b[36m[PROCESSOR] Converting document: AgentQuality-ShortSummary.docx\u001b[0m\n",
      "\u001b[32m[PROCESSOR] File validation passed: AgentQuality-ShortSummary.docx (14.86 KB)\u001b[0m\n",
      "\u001b[32m[CACHE] Cache HIT for document 27d47a85...\u001b[0m\n",
      "\u001b[32m[PROCESSOR] Using cached document\u001b[0m\n",
      "\n",
      "‚úì Conversion successful!\n",
      "   Original: AgentQuality-ShortSummary.docx\n",
      "   Pages: 1\n",
      "   Note: Intermediate PDF was created and cleaned up automatically\n"
     ]
    }
   ],
   "source": [
    "# Define path to sample DOCX\n",
    "DOCX_PATH = \"AgentQuality-ShortSummary.docx\"  # User should replace with their file\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 2: DOCX ‚Üí PDF ‚Üí DoclingDocument\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not Path(DOCX_PATH).exists():\n",
    "    print(f\"\\n‚ùå ERROR: File not found: {DOCX_PATH}\")\n",
    "    print(\"Please create a sample DOCX file or update DOCX_PATH variable\")\n",
    "else:\n",
    "    # Read file content\n",
    "    with open(DOCX_PATH, \"rb\") as f:\n",
    "        docx_content = f.read()\n",
    "    \n",
    "    print(f\"\\nüìÑ Processing DOCX: {DOCX_PATH}\")\n",
    "    print(f\"   File size: {len(docx_content) / 1024:.2f} KB\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Note: This will use LibreOffice for DOCX‚ÜíPDF conversion\")\n",
    "    \n",
    "    # Convert with Docling (includes LibreOffice conversion)\n",
    "    result = processor.convert_document(docx_content, DOCX_PATH, use_cache=True)\n",
    "    \n",
    "    docling_doc_docx = result['docling_document']\n",
    "    \n",
    "    print(f\"\\n‚úì Conversion successful!\")\n",
    "    print(f\"   Original: {result['filename']}\")\n",
    "    print(f\"   Pages: {result['page_count']}\")\n",
    "    print(f\"   Note: Intermediate PDF was created and cleaned up automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Hierarchical Pre-chunking with HybridChunker\n",
    "\n",
    "The first processing step uses Docling's **HybridChunker** to:\n",
    "- Preserve document structure (headings, paragraphs, sections)\n",
    "- Maintain page numbers and provenance data\n",
    "- Extract footnotes as separate items\n",
    "- Handle tables\n",
    "\n",
    "This creates the foundation for further splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 3: Hierarchical Pre-chunking\n",
      "================================================================================\n",
      "\u001b[36m[CHUNKER] Applying HierarchicalChunker...\u001b[0m\n",
      "\u001b[32m[CHUNKER] HierarchicalChunker produced 8 chunks\u001b[0m\n",
      "\n",
      "‚úì Hierarchical chunking complete\n",
      "   Total base chunks: 8\n",
      "\n",
      "üìã Sample Base Chunks (first 5):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Page 2 \n",
      "   Text: \"We are at the dawn of the agentic era. The transition from predictable, instruction-based tools to a...\"\n",
      "\n",
      "2. Page 3 \n",
      "   Text: \"- The Trajectory is the Truth: We must evolve beyond evaluating just the final output. The true meas...\"\n",
      "\n",
      "3. Page 3 \n",
      "   Text: \"This guide is structured to build from the \" why \" to the \" what \" and finally to the \" how .\" Use t...\"\n",
      "\n",
      "4. Page 4 \n",
      "   Text: \"- For Product Managers, Data Scientists, and QA Leaders: If you're responsible for what to measure a...\"\n",
      "\n",
      "5. Page 4 \n",
      "   Text: \"- For Team Leads and Strategists: To understand how these pieces create a selfimproving system, read...\"\n",
      "\n",
      "üìä Statistics:\n",
      "   Pages with content: 4\n",
      "   Chunks with overlap: 0\n",
      "   Average chunk length: 668 chars\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 3: Hierarchical Pre-chunking\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use the PDF document from Part 1\n",
    "if 'docling_doc' not in locals():\n",
    "    print(\"\\n‚ùå ERROR: No DoclingDocument available\")\n",
    "    print(\"Please run Part 1 first to load a PDF document\")\n",
    "else:\n",
    "    # Apply hierarchical chunking (internal method)\n",
    "    base_chunks = chunker._apply_hierarchical_chunking(docling_doc)\n",
    "    \n",
    "    print(f\"\\n‚úì Hierarchical chunking complete\")\n",
    "    print(f\"   Total base chunks: {len(base_chunks)}\")\n",
    "    \n",
    "    # Show sample chunks\n",
    "    print(f\"\\nüìã Sample Base Chunks (first 5):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, chunk in enumerate(base_chunks[:5], 1):\n",
    "        page = chunk['page_number']\n",
    "        overlap = \"‚ö†Ô∏è OVERLAP\" if chunk['is_overlap'] else \"\"\n",
    "        text_preview = chunk['text'][:100] + \"...\" if len(chunk['text']) > 100 else chunk['text']\n",
    "        \n",
    "        print(f\"\\n{i}. Page {page} {overlap}\")\n",
    "        print(f\"   Text: \\\"{text_preview}\\\"\")\n",
    "    \n",
    "    # Statistics\n",
    "    pages_with_content = set(chunk['page_number'] for chunk in base_chunks)\n",
    "    overlap_count = sum(1 for chunk in base_chunks if chunk['is_overlap'])\n",
    "    \n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"   Pages with content: {len(pages_with_content)}\")\n",
    "    print(f\"   Chunks with overlap: {overlap_count}\")\n",
    "    print(f\"   Average chunk length: {sum(len(c['text']) for c in base_chunks) / len(base_chunks):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Paragraph-level Splitting\n",
    "\n",
    "After hierarchical chunking, we apply **LangChain's RecursiveCharacterTextSplitter** to break content into paragraphs.\n",
    "\n",
    "**Configuration:**\n",
    "- chunk_size: 100 characters (configurable)\n",
    "- chunk_overlap: 10 characters\n",
    "- Separators: `\\n\\n`, `\\n`, `. `, etc.\n",
    "- keep_separator: \"end\" (preserves punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 4: Paragraph-level Splitting\n",
      "================================================================================\n",
      "\u001b[36m[CHUNKER] Applying paragraph-level splitting...\u001b[0m\n",
      "\u001b[32m[CHUNKER] Paragraph splitting produced 11 chunks\u001b[0m\n",
      "\n",
      "‚úì Paragraph splitting complete\n",
      "   Base chunks: 8\n",
      "   Paragraph chunks: 11\n",
      "   Expansion factor: 1.38x\n",
      "\n",
      "üìã Sample Paragraph Chunks (first 5):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Page 2\n",
      "   \"We are at the dawn of the agentic era. The transition from predictable, instruction-based tools to autonomous, goal-oriented AI agents presents one of...\"\n",
      "\n",
      "2. Page 3\n",
      "   \"- The Trajectory is the Truth: We must evolve beyond evaluating just the final output. The true measure of an agent's quality and safety lies in its e...\"\n",
      "\n",
      "3. Page 3\n",
      "   \"This whitepaper is for the architects, engineers, and product leaders building this future. It provides the framework to move from building capable ag...\"\n",
      "\n",
      "4. Page 3\n",
      "   \"This guide is structured to build from the \" why \" to the \" what \" and finally to the \" how .\" Use this section to navigate to the chapters most relev...\"\n",
      "\n",
      "5. Page 4\n",
      "   \"- For Product Managers, Data Scientists, and QA Leaders: If you're responsible for what to measure and how to judge quality, focus on Chapter 2: The A...\"\n",
      "\n",
      "üìä Paragraph Length Statistics:\n",
      "   Min: 197 chars\n",
      "   Max: 789 chars\n",
      "   Avg: 486 chars\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 4: Paragraph-level Splitting\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'base_chunks' not in locals():\n",
    "    print(\"\\n‚ùå ERROR: No base chunks available\")\n",
    "    print(\"Please run Part 3 first\")\n",
    "else:\n",
    "    # Apply paragraph splitting\n",
    "    paragraph_chunks = chunker._apply_paragraph_splitting(base_chunks)\n",
    "    \n",
    "    print(f\"\\n‚úì Paragraph splitting complete\")\n",
    "    print(f\"   Base chunks: {len(base_chunks)}\")\n",
    "    print(f\"   Paragraph chunks: {len(paragraph_chunks)}\")\n",
    "    print(f\"   Expansion factor: {len(paragraph_chunks) / len(base_chunks):.2f}x\")\n",
    "    \n",
    "    # Show sample paragraphs\n",
    "    print(f\"\\nüìã Sample Paragraph Chunks (first 5):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, chunk in enumerate(paragraph_chunks[:5], 1):\n",
    "        page = chunk['page_number']\n",
    "        text_preview = chunk['text'][:150] + \"...\" if len(chunk['text']) > 150 else chunk['text']\n",
    "        \n",
    "        print(f\"\\n{i}. Page {page}\")\n",
    "        print(f\"   \\\"{text_preview}\\\"\")\n",
    "    \n",
    "    # Length distribution\n",
    "    lengths = [len(chunk['text']) for chunk in paragraph_chunks]\n",
    "    print(f\"\\nüìä Paragraph Length Statistics:\")\n",
    "    print(f\"   Min: {min(lengths)} chars\")\n",
    "    print(f\"   Max: {max(lengths)} chars\")\n",
    "    print(f\"   Avg: {sum(lengths) / len(lengths):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Sentence-level Splitting with SpaCy\n",
    "\n",
    "For fine-grained verification, we use **SpaCy's sentence boundary detection** to split text into individual sentences.\n",
    "\n",
    "**Key Features:**\n",
    "- One sentence per chunk\n",
    "- Intelligent boundary detection (handles abbreviations, titles, etc.)\n",
    "- Tracks which base chunk each sentence came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 5: Sentence-level Splitting\n",
      "================================================================================\n",
      "\u001b[36m[CHUNKER] Applying sentence-level splitting with SpaCy...\u001b[0m\n",
      "\u001b[36m[CHUNKER] Loading SpaCy model for sentence splitting...\u001b[0m\n",
      "\u001b[32m[CHUNKER] SpaCy model ready for sentence splitting\u001b[0m\n",
      "\u001b[32m[CHUNKER] Sentence splitting produced 48 individual sentences\u001b[0m\n",
      "\n",
      "‚úì Sentence splitting complete\n",
      "   Base chunks: 8\n",
      "   Sentence chunks: 48\n",
      "   Expansion factor: 6.00x\n",
      "\n",
      "üìã Sample Sentence Chunks (first 10):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Page 2 (Base Chunk 0)\n",
      "   \"We are at the dawn of the agentic era.\"\n",
      "\n",
      "2. Page 2 (Base Chunk 0)\n",
      "   \"The transition from predictable, instruction-based tools to autonomous, goal-oriented AI agents presents one of the most profound shifts in software engineering in decades.\"\n",
      "\n",
      "3. Page 2 (Base Chunk 0)\n",
      "   \"While these agents unlock incredible capabilities, their inherent non-determinism makes them unpredictable and shatters our traditional models of quality assurance.\"\n",
      "\n",
      "4. Page 2 (Base Chunk 0)\n",
      "   \"This whitepaper serves as a practical guide to this new reality, founded on a simple but radical principle:\n",
      "Agent quality is an architectural pillar, not a final testing phase.\"\n",
      "\n",
      "5. Page 2 (Base Chunk 0)\n",
      "   \"This guide is built on three core messages:\"\n",
      "\n",
      "6. Page 3 (Base Chunk 1)\n",
      "   \"- The Trajectory is the Truth: We must evolve beyond evaluating just the final output.\"\n",
      "\n",
      "7. Page 3 (Base Chunk 1)\n",
      "   \"The true measure of an agent's quality and safety lies in its entire decision-making process.\"\n",
      "\n",
      "8. Page 3 (Base Chunk 1)\n",
      "   \"- Observability is the Foundation: You cannot judge a process you cannot see.\"\n",
      "\n",
      "9. Page 3 (Base Chunk 1)\n",
      "   \"We detail the \"three pillars\" of observability - Logging , Tracing , and Metrics - as the essential technical foundation for capturing the agent's \"thought process.\"\n",
      "\n",
      "10. Page 3 (Base Chunk 1)\n",
      "   \"\"\n",
      "- Evaluation is a Continuous Loop: We synthesize these concepts into the \"Agent Quality Flywheel\" , an operational playbook for turning this data into actionable insights.\"\n",
      "\n",
      "üìä Sentence Distribution:\n",
      "   Total sentences: 48\n",
      "   Avg sentences per base chunk: 6.0\n",
      "   Base chunk with most sentences: 9 sentences\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 5: Sentence-level Splitting\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'base_chunks' not in locals():\n",
    "    print(\"\\n‚ùå ERROR: No base chunks available\")\n",
    "    print(\"Please run Part 3 first\")\n",
    "else:\n",
    "    # Apply sentence splitting\n",
    "    sentence_chunks = chunker._apply_sentence_splitting(base_chunks)\n",
    "    \n",
    "    print(f\"\\n‚úì Sentence splitting complete\")\n",
    "    print(f\"   Base chunks: {len(base_chunks)}\")\n",
    "    print(f\"   Sentence chunks: {len(sentence_chunks)}\")\n",
    "    print(f\"   Expansion factor: {len(sentence_chunks) / len(base_chunks):.2f}x\")\n",
    "    \n",
    "    # Show sample sentences\n",
    "    print(f\"\\nüìã Sample Sentence Chunks (first 10):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, chunk in enumerate(sentence_chunks[:10], 1):\n",
    "        page = chunk['page_number']\n",
    "        base_idx = chunk.get('base_chunk_index', 'N/A')\n",
    "        text = chunk['text']\n",
    "        \n",
    "        print(f\"\\n{i}. Page {page} (Base Chunk {base_idx})\")\n",
    "        print(f\"   \\\"{text}\\\"\")\n",
    "    \n",
    "    # Sentences per base chunk\n",
    "    from collections import Counter\n",
    "    base_chunk_counts = Counter(chunk.get('base_chunk_index', -1) for chunk in sentence_chunks)\n",
    "    \n",
    "    print(f\"\\nüìä Sentence Distribution:\")\n",
    "    print(f\"   Total sentences: {len(sentence_chunks)}\")\n",
    "    print(f\"   Avg sentences per base chunk: {len(sentence_chunks) / len(base_chunks):.1f}\")\n",
    "    print(f\"   Base chunk with most sentences: {max(base_chunk_counts.values())} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Creating Verification Shells (DocumentChunk Objects)\n",
    "\n",
    "The final step assigns **item numbers** to each chunk and creates **DocumentChunk** objects ready for verification.\n",
    "\n",
    "**Item Numbering:**\n",
    "- **Paragraph mode:** Simple sequential (1, 2, 3...) - resets per page\n",
    "- **Sentence mode:** Hierarchical (1.1, 1.2, 2.1, 2.2...) - shows base chunk relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 6: Creating DocumentChunk Objects (Paragraph Mode)\n",
      "================================================================================\n",
      "\u001b[36m[CHUNKER] Chunking document in paragraph mode...\u001b[0m\n",
      "\u001b[36m[CHUNKER] Applying HierarchicalChunker...\u001b[0m\n",
      "\u001b[32m[CHUNKER] HierarchicalChunker produced 8 chunks\u001b[0m\n",
      "\u001b[36m[CHUNKER] Applying paragraph-level splitting...\u001b[0m\n",
      "\u001b[32m[CHUNKER] Paragraph splitting produced 11 chunks\u001b[0m\n",
      "\u001b[36m[CHUNKER] Assigning item numbers (paragraph mode)...\u001b[0m\n",
      "\u001b[32m[CHUNKER] Assigned item numbers to 11 chunks\u001b[0m\n",
      "\u001b[32m[CHUNKER] Chunking complete: 11 total chunks\u001b[0m\n",
      "\n",
      "‚úì Document chunks created\n",
      "   Total chunks: 11\n",
      "   Mode: paragraph\n",
      "\n",
      "üìã Document Chunks (first 10):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Page 2, Item 1\n",
      "  \"We are at the dawn of the agentic era. The transition from predictable, instruct...\"\n",
      "\n",
      "Page 3, Item 1\n",
      "  \"- The Trajectory is the Truth: We must evolve beyond evaluating just the final o...\"\n",
      "\n",
      "Page 3, Item 2\n",
      "  \"This whitepaper is for the architects, engineers, and product leaders building t...\"\n",
      "\n",
      "Page 3, Item 3\n",
      "  \"This guide is structured to build from the \" why \" to the \" what \" and finally t...\"\n",
      "\n",
      "Page 4, Item 1\n",
      "  \"- For Product Managers, Data Scientists, and QA Leaders: If you're responsible f...\"\n",
      "\n",
      "Page 4, Item 2\n",
      "  \"- For Team Leads and Strategists: To understand how these pieces create a selfim...\"\n",
      "\n",
      "Page 4, Item 3\n",
      "  \"The world of artificial intelligence is transforming at full speed. We are movin...\"\n",
      "\n",
      "Page 4, Item 4\n",
      "  \"To understand this shift, compare traditional software to a delivery truck and a...\"\n",
      "\n",
      "Page 5, Item 1\n",
      "  \"This evolution is fundamentally changing how we must approach software quality. ...\"\n",
      "\n",
      "Page 5, Item 2\n",
      "  \"This chapter inspects this new paradigm. We will explore why agent quality deman...\"\n",
      "\n",
      "üìä Distribution by Page:\n",
      "   Page 2: 1 chunks\n",
      "   Page 3: 3 chunks\n",
      "   Page 4: 4 chunks\n",
      "   Page 5: 3 chunks\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 6: Creating DocumentChunk Objects (Paragraph Mode)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'docling_doc' not in locals():\n",
    "    print(\"\\n‚ùå ERROR: No DoclingDocument available\")\n",
    "else:\n",
    "    # Chunk in paragraph mode\n",
    "    paragraph_doc_chunks = chunker.chunk_document(\n",
    "        docling_doc, \n",
    "        mode=ChunkingMode.PARAGRAPH\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Document chunks created\")\n",
    "    print(f\"   Total chunks: {len(paragraph_doc_chunks)}\")\n",
    "    print(f\"   Mode: {ChunkingMode.PARAGRAPH.value}\")\n",
    "    \n",
    "    # Show first 10 chunks with full metadata\n",
    "    print(f\"\\nüìã Document Chunks (first 10):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for chunk in paragraph_doc_chunks[:10]:\n",
    "        overlap_flag = \" [OVERLAP]\" if chunk.is_overlap else \"\"\n",
    "        text_preview = chunk.text[:80] + \"...\" if len(chunk.text) > 80 else chunk.text\n",
    "        \n",
    "        print(f\"\\nPage {chunk.page_number}, Item {chunk.item_number}{overlap_flag}\")\n",
    "        print(f\"  \\\"{text_preview}\\\"\")\n",
    "    \n",
    "    # Page distribution\n",
    "    from collections import defaultdict\n",
    "    chunks_per_page = defaultdict(int)\n",
    "    for chunk in paragraph_doc_chunks:\n",
    "        chunks_per_page[chunk.page_number] += 1\n",
    "    \n",
    "    print(f\"\\nüìä Distribution by Page:\")\n",
    "    for page in sorted(chunks_per_page.keys())[:5]:  # First 5 pages\n",
    "        print(f\"   Page {page}: {chunks_per_page[page]} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 6b: Creating DocumentChunk Objects (Sentence Mode)\n",
      "================================================================================\n",
      "\u001b[36m[CHUNKER] Chunking document in sentence mode...\u001b[0m\n",
      "\u001b[36m[CHUNKER] Applying HierarchicalChunker...\u001b[0m\n",
      "\u001b[32m[CHUNKER] HierarchicalChunker produced 8 chunks\u001b[0m\n",
      "\u001b[36m[CHUNKER] Applying sentence-level splitting with SpaCy...\u001b[0m\n",
      "\u001b[32m[CHUNKER] Sentence splitting produced 48 individual sentences\u001b[0m\n",
      "\u001b[36m[CHUNKER] Assigning item numbers (sentence mode)...\u001b[0m\n",
      "\u001b[32m[CHUNKER] Assigned item numbers to 48 chunks\u001b[0m\n",
      "\u001b[32m[CHUNKER] Chunking complete: 48 total chunks\u001b[0m\n",
      "\n",
      "‚úì Document chunks created\n",
      "   Total chunks: 48\n",
      "   Mode: sentence\n",
      "\n",
      "üìã Document Chunks with Hierarchical Numbering (first 15):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Page 2, Item 1.1\n",
      "  \"We are at the dawn of the agentic era.\"\n",
      "\n",
      "Page 2, Item 1.2\n",
      "  \"The transition from predictable, instruction-based tools to autonomous, goal-oriented AI agents presents one of the most profound shifts in software engineering in decades.\"\n",
      "\n",
      "Page 2, Item 1.3\n",
      "  \"While these agents unlock incredible capabilities, their inherent non-determinism makes them unpredictable and shatters our traditional models of quality assurance.\"\n",
      "\n",
      "Page 2, Item 1.4\n",
      "  \"This whitepaper serves as a practical guide to this new reality, founded on a simple but radical principle:\n",
      "Agent quality is an architectural pillar, not a final testing phase.\"\n",
      "\n",
      "Page 2, Item 1.5\n",
      "  \"This guide is built on three core messages:\"\n",
      "\n",
      "Page 3, Item 1.1\n",
      "  \"- The Trajectory is the Truth: We must evolve beyond evaluating just the final output.\"\n",
      "\n",
      "Page 3, Item 1.2\n",
      "  \"The true measure of an agent's quality and safety lies in its entire decision-making process.\"\n",
      "\n",
      "Page 3, Item 1.3\n",
      "  \"- Observability is the Foundation: You cannot judge a process you cannot see.\"\n",
      "\n",
      "Page 3, Item 1.4\n",
      "  \"We detail the \"three pillars\" of observability - Logging , Tracing , and Metrics - as the essential technical foundation for capturing the agent's \"thought process.\"\n",
      "\n",
      "Page 3, Item 1.5\n",
      "  \"\"\n",
      "- Evaluation is a Continuous Loop: We synthesize these concepts into the \"Agent Quality Flywheel\" , an operational playbook for turning this data into actionable insights.\"\n",
      "\n",
      "Page 3, Item 1.6\n",
      "  \"This system uses a hybrid of scalable AI-driven evaluators and indispensable Human-in-theLoop (HITL) judgment to drive relentless improvement.\"\n",
      "\n",
      "Page 3, Item 1.7\n",
      "  \"This whitepaper is for the architects, engineers, and product leaders building this future.\"\n",
      "\n",
      "Page 3, Item 1.8\n",
      "  \"It provides the framework to move from building capable agents to building reliable and trustworthy ones.\"\n",
      "\n",
      "Page 3, Item 2.1\n",
      "  \"This guide is structured to build from the \" why \" to the \" what \" and finally to the \" how .\"\"\n",
      "\n",
      "Page 3, Item 2.2\n",
      "  \"Use this section to navigate to the chapters most relevant to your role.\n",
      "-\"\n",
      "\n",
      "üìä Hierarchical Structure Analysis:\n",
      "   Total sentences: 48\n",
      "   Total base chunks (paragraph-level): 3\n",
      "   Avg sentences per base chunk: 16.0\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 6b: Creating DocumentChunk Objects (Sentence Mode)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'docling_doc' not in locals():\n",
    "    print(\"\\n‚ùå ERROR: No DoclingDocument available\")\n",
    "else:\n",
    "    # Chunk in sentence mode\n",
    "    sentence_doc_chunks = chunker.chunk_document(\n",
    "        docling_doc, \n",
    "        mode=ChunkingMode.SENTENCE\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Document chunks created\")\n",
    "    print(f\"   Total chunks: {len(sentence_doc_chunks)}\")\n",
    "    print(f\"   Mode: {ChunkingMode.SENTENCE.value}\")\n",
    "    \n",
    "    # Show first 15 chunks with hierarchical numbering\n",
    "    print(f\"\\nüìã Document Chunks with Hierarchical Numbering (first 15):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for chunk in sentence_doc_chunks[:15]:\n",
    "        overlap_flag = \" [OVERLAP]\" if chunk.is_overlap else \"\"\n",
    "        \n",
    "        print(f\"\\nPage {chunk.page_number}, Item {chunk.item_number}{overlap_flag}\")\n",
    "        print(f\"  \\\"{chunk.text}\\\"\")\n",
    "    \n",
    "    # Analyze hierarchical structure\n",
    "    print(f\"\\nüìä Hierarchical Structure Analysis:\")\n",
    "    \n",
    "    # Count base chunks (items like 1.x, 2.x, 3.x)\n",
    "    base_items = set()\n",
    "    for chunk in sentence_doc_chunks:\n",
    "        if '.' in chunk.item_number:\n",
    "            base_items.add(chunk.item_number.split('.')[0])\n",
    "    \n",
    "    print(f\"   Total sentences: {len(sentence_doc_chunks)}\")\n",
    "    print(f\"   Total base chunks (paragraph-level): {len(base_items)}\")\n",
    "    print(f\"   Avg sentences per base chunk: {len(sentence_doc_chunks) / len(base_items):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparing Chunking Modes\n",
    "\n",
    "Let's compare the two chunking modes side-by-side to understand their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 7: Chunking Mode Comparison\n",
      "================================================================================\n",
      "\n",
      "Metric                                   Paragraph       Sentence       \n",
      "----------------------------------------------------------------------\n",
      "Total chunks                             11              48             \n",
      "Avg chunk length (chars)                 486             111            \n",
      "Avg chunks per page                      2.8             12.0           \n",
      "Chunks with overlap flag                 0               0              \n",
      "\n",
      "üí° Recommendations:\n",
      "   ‚Ä¢ Use PARAGRAPH mode for: General document verification, faster processing\n",
      "   ‚Ä¢ Use SENTENCE mode for: Fine-grained verification, detailed fact-checking\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 7: Chunking Mode Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'paragraph_doc_chunks' in locals() and 'sentence_doc_chunks' in locals():\n",
    "    print(f\"\\n{'Metric':<40} {'Paragraph':<15} {'Sentence':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    print(f\"{'Total chunks':<40} {len(paragraph_doc_chunks):<15} {len(sentence_doc_chunks):<15}\")\n",
    "    \n",
    "    # Average text length\n",
    "    para_avg_len = sum(len(c.text) for c in paragraph_doc_chunks) / len(paragraph_doc_chunks)\n",
    "    sent_avg_len = sum(len(c.text) for c in sentence_doc_chunks) / len(sentence_doc_chunks)\n",
    "    print(f\"{'Avg chunk length (chars)':<40} {para_avg_len:<15.0f} {sent_avg_len:<15.0f}\")\n",
    "    \n",
    "    # Chunks per page (average)\n",
    "    para_pages = set(c.page_number for c in paragraph_doc_chunks)\n",
    "    sent_pages = set(c.page_number for c in sentence_doc_chunks)\n",
    "    para_per_page = len(paragraph_doc_chunks) / len(para_pages)\n",
    "    sent_per_page = len(sentence_doc_chunks) / len(sent_pages)\n",
    "    print(f\"{'Avg chunks per page':<40} {para_per_page:<15.1f} {sent_per_page:<15.1f}\")\n",
    "    \n",
    "    # Overlap counts\n",
    "    para_overlap = sum(1 for c in paragraph_doc_chunks if c.is_overlap)\n",
    "    sent_overlap = sum(1 for c in sentence_doc_chunks if c.is_overlap)\n",
    "    print(f\"{'Chunks with overlap flag':<40} {para_overlap:<15} {sent_overlap:<15}\")\n",
    "    \n",
    "    print(f\"\\nüí° Recommendations:\")\n",
    "    print(f\"   ‚Ä¢ Use PARAGRAPH mode for: General document verification, faster processing\")\n",
    "    print(f\"   ‚Ä¢ Use SENTENCE mode for: Fine-grained verification, detailed fact-checking\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå Please run Part 6 and 6b first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Exporting Verification Shell\n",
    "\n",
    "Now that we have DocumentChunk objects, let's see how they would be exported for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PART 8: Exporting DocumentChunks to JSON\n",
      "================================================================================\n",
      "\n",
      "üìÑ Sample Export (first 5 chunks):\n",
      "[\n",
      "  {\n",
      "    \"page_number\": 2,\n",
      "    \"item_number\": \"1\",\n",
      "    \"text\": \"We are at the dawn of the agentic era. The transition from predictable, instruction-based tools to autonomous, goal-oriented AI agents presents one of the most profound shifts in software engineering in decades. While these agents unlock incredible capabilities, their inherent non-determinism makes them unpredictable and shatters our traditional models of quality assurance.\\nThis whitepaper serves as a practical guide to this new reality, founded on a simple but radical principle:\\nAgent quality is an architectural pillar, not a final testing phase.\\nThis guide is built on three core messages:\",\n",
      "    \"is_overlap\": false,\n",
      "    \"verified\": null,\n",
      "    \"verification_score\": null,\n",
      "    \"verification_source\": \"\",\n",
      "    \"verification_note\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"page_number\": 3,\n",
      "    \"item_number\": \"1\",\n",
      "    \"text\": \"- The Trajectory is the Truth: We must evolve beyond evaluating just the final output. The true measure of an agent's quality and safety lies in its entire decision-making process.\\n- Observability is the Foundation: You cannot judge a process you cannot see. We detail the \\\"three pillars\\\" of observability - Logging , Tracing , and Metrics - as the essential technical foundation for capturing the agent's \\\"thought process.\\\"\\n- Evaluation is a Continuous Loop: We synthesize these concepts into the \\\"Agent Quality Flywheel\\\" , an operational playbook for turning this data into actionable insights. This system uses a hybrid of scalable AI-driven evaluators and indispensable Human-in-theLoop (HITL) judgment to drive relentless improvement.\",\n",
      "    \"is_overlap\": false,\n",
      "    \"verified\": null,\n",
      "    \"verification_score\": null,\n",
      "    \"verification_source\": \"\",\n",
      "    \"verification_note\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"page_number\": 3,\n",
      "    \"item_number\": \"2\",\n",
      "    \"text\": \"This whitepaper is for the architects, engineers, and product leaders building this future. It provides the framework to move from building capable agents to building reliable and trustworthy ones.\",\n",
      "    \"is_overlap\": false,\n",
      "    \"verified\": null,\n",
      "    \"verification_score\": null,\n",
      "    \"verification_source\": \"\",\n",
      "    \"verification_note\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"page_number\": 3,\n",
      "    \"item_number\": \"3\",\n",
      "    \"text\": \"This guide is structured to build from the \\\" why \\\" to the \\\" what \\\" and finally to the \\\" how .\\\" Use this section to navigate to the chapters most relevant to your role.\\n- For All Readers: Start with Chapter 1: Agent Quality in a Non-Deterministic World . This chapter establishes the core problem. It explains why traditional QA fails for AI agents and introduces the Four Pillars of Agent Quality (Effectiveness, Efficiency, Robustness, and Safety) that define our goals.\",\n",
      "    \"is_overlap\": false,\n",
      "    \"verified\": null,\n",
      "    \"verification_score\": null,\n",
      "    \"verification_source\": \"\",\n",
      "    \"verification_note\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"page_number\": 4,\n",
      "    \"item_number\": \"1\",\n",
      "    \"text\": \"- For Product Managers, Data Scientists, and QA Leaders: If you're responsible for what to measure and how to judge quality, focus on Chapter 2: The Art of Agent Evaluation . This chapter is your strategic guide. It details the \\\"Outside-In\\\" hierarchy for evaluation, explains the scalable \\\"LLM-as-a-Judge\\\" paradigm , and clarifies the critical role of Human-in-the-Loop (HITL) evaluation.\\n- For Engineers, Architects, and SREs: If you build the systems, your technical blueprint is Chapter 3: Observability . This chapter moves from theory to implementation. It provides the \\\"kitchen analogy\\\" (Line Cook vs. Gourmet Chef) to explain monitoring vs. observability and details the Three Pillars of Observability: Logs, Traces, and Metrics - the tools you need to build an \\\"evaluatable\\\" agent.\",\n",
      "    \"is_overlap\": false,\n",
      "    \"verified\": null,\n",
      "    \"verification_score\": null,\n",
      "    \"verification_source\": \"\",\n",
      "    \"verification_note\": \"\"\n",
      "  }\n",
      "]\n",
      "\n",
      "‚úì Exported 11 chunks to: verification_shell.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 8: Exporting DocumentChunks to JSON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'paragraph_doc_chunks' in locals():\n",
    "    # Convert to JSON-serializable format\n",
    "    chunks_data = [\n",
    "        {\n",
    "            \"page_number\": chunk.page_number,\n",
    "            \"item_number\": chunk.item_number,\n",
    "            \"text\": chunk.text,\n",
    "            \"is_overlap\": chunk.is_overlap,\n",
    "            \"verified\": None,  # To be filled by AI\n",
    "            \"verification_score\": None,\n",
    "            \"verification_source\": \"\",\n",
    "            \"verification_note\": \"\"\n",
    "        }\n",
    "        for chunk in paragraph_doc_chunks[:5]  # First 5 for demo\n",
    "    ]\n",
    "    \n",
    "    # Pretty print JSON\n",
    "    print(\"\\nüìÑ Sample Export (first 5 chunks):\")\n",
    "    print(json.dumps(chunks_data, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = \"verification_shell.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(\n",
    "            [chunk.model_dump() for chunk in paragraph_doc_chunks], \n",
    "            f, \n",
    "            indent=2, \n",
    "            ensure_ascii=False\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n‚úì Exported {len(paragraph_doc_chunks)} chunks to: {output_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå No chunks available for export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete document processing pipeline:\n",
    "\n",
    "1. ‚úÖ **PDF Conversion** - Docling converts PDF to structured DoclingDocument\n",
    "2. ‚úÖ **DOCX Conversion** - LibreOffice ‚Üí PDF ‚Üí DoclingDocument for accurate pagination\n",
    "3. ‚úÖ **Hierarchical Chunking** - HybridChunker preserves document structure\n",
    "4. ‚úÖ **Paragraph Splitting** - RecursiveCharacterTextSplitter for paragraph-level chunks\n",
    "5. ‚úÖ **Sentence Splitting** - SpaCy sentencizer for fine-grained chunks\n",
    "6. ‚úÖ **DocumentChunk Creation** - Structured objects with page/item metadata\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Docling** provides robust PDF/DOCX parsing with metadata preservation\n",
    "- **HybridChunker** maintains document hierarchy while chunking\n",
    "- **Paragraph mode** creates ~100-char chunks with simple numbering (1, 2, 3...)\n",
    "- **Sentence mode** creates true sentence-level chunks with hierarchical numbering (1.1, 1.2...)\n",
    "- **DocumentChunk objects** are ready for AI verification with Gemini\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore `gemini_features.ipynb` to see AI verification in action\n",
    "- Run the full application: `./start_all.sh`\n",
    "- Check `backend/app/processing/` for implementation details\n",
    "- Try different documents and chunking modes to understand behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quvnvela8of",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Restart the kernel to reload the updated document_processor module\n",
    "# import IPython\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
