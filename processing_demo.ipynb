{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing Demo\n",
    "\n",
    "This notebook demonstrates the document processing pipeline used in the Content Verification Tool:\n",
    "\n",
    "1. **Docling PDF Conversion** - Converting PDF to DoclingDocument\n",
    "2. **Docling DOCX Conversion** - Converting DOCX to PDF then DoclingDocument (with LibreOffice)\n",
    "3. **Hierarchical Pre-chunking** - Using HybridChunker from Docling\n",
    "4. **Paragraph-level Splitting** - Using LangChain RecursiveCharacterTextSplitter\n",
    "5. **Sentence-level Splitting** - Using SpaCy sentence detection\n",
    "6. **Verification Shell Creation** - Creating DocumentChunk objects with page # and item # assignments\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install dependencies (already in pyproject.toml)\n",
    "pip install docling docling-core langchain-text-splitters spacy python-docx termcolor\n",
    "\n",
    "# Download SpaCy model\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "## Sample Documents\n",
    "\n",
    "This demo requires sample PDF and DOCX files. You can:\n",
    "- Use your own legal documents (max 10MB)\n",
    "- Create sample files named `sample.pdf` and `sample.docx` in the project root\n",
    "- Update the file paths in the code cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Initialize Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# Add backend to path for imports\n",
    "sys.path.insert(0, str(Path.cwd() / 'backend'))\n",
    "\n",
    "from app.processing.document_processor import DocumentProcessor\n",
    "from app.processing.chunker import DocumentChunker\n",
    "from app.models import ChunkingMode, DocumentChunk\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INITIALIZING DOCUMENT PROCESSING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize processor and chunker\n",
    "processor = DocumentProcessor()\n",
    "chunker = DocumentChunker()\n",
    "\n",
    "print(\"\\n‚úì All components initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: PDF Processing with Docling\n",
    "\n",
    "We'll convert a PDF document to a DoclingDocument object, which preserves:\n",
    "- Page structure and numbers\n",
    "- Paragraphs and sections\n",
    "- Tables\n",
    "- Footnotes\n",
    "- Provenance data (for tracking text spans across pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to sample PDF\n",
    "PDF_PATH = \"sample.pdf\"  # User should replace with their file\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 1: PDF ‚Üí DoclingDocument\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if file exists\n",
    "if not Path(PDF_PATH).exists():\n",
    "    print(f\"\\n‚ùå ERROR: File not found: {PDF_PATH}\")\n",
    "    print(\"Please create a sample PDF file or update PDF_PATH variable\")\n",
    "else:\n",
    "    # Read file content\n",
    "    with open(PDF_PATH, \"rb\") as f:\n",
    "        pdf_content = f.read()\n",
    "    \n",
    "    print(f\"\\nüìÑ Processing PDF: {PDF_PATH}\")\n",
    "    print(f\"   File size: {len(pdf_content) / 1024:.2f} KB\")\n",
    "    \n",
    "    # Convert with Docling\n",
    "    result = processor.convert_document(pdf_content, PDF_PATH, use_cache=True)\n",
    "    \n",
    "    docling_doc = result['docling_document']\n",
    "    \n",
    "    print(f\"\\n‚úì Conversion successful!\")\n",
    "    print(f\"   Filename: {result['filename']}\")\n",
    "    print(f\"   Pages: {result['page_count']}\")\n",
    "    print(f\"   File size: {result['file_size']} bytes\")\n",
    "    \n",
    "    # Inspect DoclingDocument structure\n",
    "    print(f\"\\nüìä DoclingDocument Structure:\")\n",
    "    print(f\"   Type: {type(docling_doc)}\")\n",
    "    print(f\"   Has pages: {hasattr(docling_doc, 'pages')}\")\n",
    "    if hasattr(docling_doc, 'pages'):\n",
    "        print(f\"   Total pages: {len(docling_doc.pages)}\")\n",
    "        print(f\"   First page: {docling_doc.pages[0] if docling_doc.pages else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DOCX Processing with LibreOffice + Docling\n",
    "\n",
    "DOCX files are converted to PDF using LibreOffice first, then processed with Docling.\n",
    "\n",
    "**Why?**\n",
    "- Accurate page numbers (DOCX doesn't have fixed pages)\n",
    "- Consistent processing pipeline (everything goes through PDF)\n",
    "- Better metadata extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to sample DOCX\n",
    "DOCX_PATH = \"sample.docx\"  # User should replace with their file\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 2: DOCX ‚Üí PDF ‚Üí DoclingDocument\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not Path(DOCX_PATH).exists():\n",
    "    print(f\"\\n‚ùå ERROR: File not found: {DOCX_PATH}\")\n",
    "    print(\"Please create a sample DOCX file or update DOCX_PATH variable\")\n",
    "else:\n",
    "    # Read file content\n",
    "    with open(DOCX_PATH, \"rb\") as f:\n",
    "        docx_content = f.read()\n",
    "    \n",
    "    print(f\"\\nüìÑ Processing DOCX: {DOCX_PATH}\")\n",
    "    print(f\"   File size: {len(docx_content) / 1024:.2f} KB\")\n",
    "    print(f\"\\n‚ö†Ô∏è  Note: This will use LibreOffice for DOCX‚ÜíPDF conversion\")\n",
    "    \n",
    "    # Convert with Docling (includes LibreOffice conversion)\n",
    "    result = processor.convert_document(docx_content, DOCX_PATH, use_cache=True)\n",
    "    \n",
    "    docling_doc_docx = result['docling_document']\n",
    "    \n",
    "    print(f\"\\n‚úì Conversion successful!\")\n",
    "    print(f\"   Original: {result['filename']}\")\n",
    "    print(f\"   Pages: {result['page_count']}\")\n",
    "    print(f\"   Note: Intermediate PDF was created and cleaned up automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Hierarchical Pre-chunking with HybridChunker\n",
    "\n",
    "The first processing step uses Docling's **HybridChunker** to:\n",
    "- Preserve document structure (headings, paragraphs, sections)\n",
    "- Maintain page numbers and provenance data\n",
    "- Extract footnotes as separate items\n",
    "- Handle tables\n",
    "\n",
    "This creates the foundation for further splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 3: Hierarchical Pre-chunking\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use the PDF document from Part 1\n",
    "if 'docling_doc' not in locals():\n",
    "    print(\"\\n‚ùå ERROR: No DoclingDocument available\")\n",
    "    print(\"Please run Part 1 first to load a PDF document\")\n",
    "else:\n",
    "    # Apply hierarchical chunking (internal method)\n",
    "    base_chunks = chunker._apply_hierarchical_chunking(docling_doc)\n",
    "    \n",
    "    print(f\"\\n‚úì Hierarchical chunking complete\")\n",
    "    print(f\"   Total base chunks: {len(base_chunks)}\")\n",
    "    \n",
    "    # Show sample chunks\n",
    "    print(f\"\\nüìã Sample Base Chunks (first 5):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, chunk in enumerate(base_chunks[:5], 1):\n",
    "        page = chunk['page_number']\n",
    "        overlap = \"‚ö†Ô∏è OVERLAP\" if chunk['is_overlap'] else \"\"\n",
    "        text_preview = chunk['text'][:100] + \"...\" if len(chunk['text']) > 100 else chunk['text']\n",
    "        \n",
    "        print(f\"\\n{i}. Page {page} {overlap}\")\n",
    "        print(f\"   Text: \\\"{text_preview}\\\"\")\n",
    "    \n",
    "    # Statistics\n",
    "    pages_with_content = set(chunk['page_number'] for chunk in base_chunks)\n",
    "    overlap_count = sum(1 for chunk in base_chunks if chunk['is_overlap'])\n",
    "    \n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"   Pages with content: {len(pages_with_content)}\")\n",
    "    print(f\"   Chunks with overlap: {overlap_count}\")\n",
    "    print(f\"   Average chunk length: {sum(len(c['text']) for c in base_chunks) / len(base_chunks):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Paragraph-level Splitting\n",
    "\n",
    "After hierarchical chunking, we apply **LangChain's RecursiveCharacterTextSplitter** to break content into paragraphs.\n",
    "\n",
    "**Configuration:**\n",
    "- chunk_size: 100 characters (configurable)\n",
    "- chunk_overlap: 10 characters\n",
    "- Separators: `\\n\\n`, `\\n`, `. `, etc.\n",
    "- keep_separator: \"end\" (preserves punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 4: Paragraph-level Splitting\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'base_chunks' not in locals():\n",
    "    print(\"\\n‚ùå ERROR: No base chunks available\")\n",
    "    print(\"Please run Part 3 first\")\n",
    "else:\n",
    "    # Apply paragraph splitting\n",
    "    paragraph_chunks = chunker._apply_paragraph_splitting(base_chunks)\n",
    "    \n",
    "    print(f\"\\n‚úì Paragraph splitting complete\")\n",
    "    print(f\"   Base chunks: {len(base_chunks)}\")\n",
    "    print(f\"   Paragraph chunks: {len(paragraph_chunks)}\")\n",
    "    print(f\"   Expansion factor: {len(paragraph_chunks) / len(base_chunks):.2f}x\")\n",
    "    \n",
    "    # Show sample paragraphs\n",
    "    print(f\"\\nüìã Sample Paragraph Chunks (first 5):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, chunk in enumerate(paragraph_chunks[:5], 1):\n",
    "        page = chunk['page_number']\n",
    "        text_preview = chunk['text'][:150] + \"...\" if len(chunk['text']) > 150 else chunk['text']\n",
    "        \n",
    "        print(f\"\\n{i}. Page {page}\")\n",
    "        print(f\"   \\\"{text_preview}\\\"\")\n",
    "    \n",
    "    # Length distribution\n",
    "    lengths = [len(chunk['text']) for chunk in paragraph_chunks]\n",
    "    print(f\"\\nüìä Paragraph Length Statistics:\")\n",
    "    print(f\"   Min: {min(lengths)} chars\")\n",
    "    print(f\"   Max: {max(lengths)} chars\")\n",
    "    print(f\"   Avg: {sum(lengths) / len(lengths):.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Sentence-level Splitting with SpaCy\n",
    "\n",
    "For fine-grained verification, we use **SpaCy's sentence boundary detection** to split text into individual sentences.\n",
    "\n",
    "**Key Features:**\n",
    "- One sentence per chunk\n",
    "- Intelligent boundary detection (handles abbreviations, titles, etc.)\n",
    "- Tracks which base chunk each sentence came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 5: Sentence-level Splitting\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'base_chunks' not in locals():\n",
    "    print(\"\\n‚ùå ERROR: No base chunks available\")\n",
    "    print(\"Please run Part 3 first\")\n",
    "else:\n",
    "    # Apply sentence splitting\n",
    "    sentence_chunks = chunker._apply_sentence_splitting(base_chunks)\n",
    "    \n",
    "    print(f\"\\n‚úì Sentence splitting complete\")\n",
    "    print(f\"   Base chunks: {len(base_chunks)}\")\n",
    "    print(f\"   Sentence chunks: {len(sentence_chunks)}\")\n",
    "    print(f\"   Expansion factor: {len(sentence_chunks) / len(base_chunks):.2f}x\")\n",
    "    \n",
    "    # Show sample sentences\n",
    "    print(f\"\\nüìã Sample Sentence Chunks (first 10):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, chunk in enumerate(sentence_chunks[:10], 1):\n",
    "        page = chunk['page_number']\n",
    "        base_idx = chunk.get('base_chunk_index', 'N/A')\n",
    "        text = chunk['text']\n",
    "        \n",
    "        print(f\"\\n{i}. Page {page} (Base Chunk {base_idx})\")\n",
    "        print(f\"   \\\"{text}\\\"\")\n",
    "    \n",
    "    # Sentences per base chunk\n",
    "    from collections import Counter\n",
    "    base_chunk_counts = Counter(chunk.get('base_chunk_index', -1) for chunk in sentence_chunks)\n",
    "    \n",
    "    print(f\"\\nüìä Sentence Distribution:\")\n",
    "    print(f\"   Total sentences: {len(sentence_chunks)}\")\n",
    "    print(f\"   Avg sentences per base chunk: {len(sentence_chunks) / len(base_chunks):.1f}\")\n",
    "    print(f\"   Base chunk with most sentences: {max(base_chunk_counts.values())} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Creating Verification Shells (DocumentChunk Objects)\n",
    "\n",
    "The final step assigns **item numbers** to each chunk and creates **DocumentChunk** objects ready for verification.\n",
    "\n",
    "**Item Numbering:**\n",
    "- **Paragraph mode:** Simple sequential (1, 2, 3...) - resets per page\n",
    "- **Sentence mode:** Hierarchical (1.1, 1.2, 2.1, 2.2...) - shows base chunk relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 6: Creating DocumentChunk Objects (Paragraph Mode)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'docling_doc' not in locals():\n",
    "    print(\"\\n‚ùå ERROR: No DoclingDocument available\")\n",
    "else:\n",
    "    # Chunk in paragraph mode\n",
    "    paragraph_doc_chunks = chunker.chunk_document(\n",
    "        docling_doc, \n",
    "        mode=ChunkingMode.PARAGRAPH\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Document chunks created\")\n",
    "    print(f\"   Total chunks: {len(paragraph_doc_chunks)}\")\n",
    "    print(f\"   Mode: {ChunkingMode.PARAGRAPH.value}\")\n",
    "    \n",
    "    # Show first 10 chunks with full metadata\n",
    "    print(f\"\\nüìã Document Chunks (first 10):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for chunk in paragraph_doc_chunks[:10]:\n",
    "        overlap_flag = \" [OVERLAP]\" if chunk.is_overlap else \"\"\n",
    "        text_preview = chunk.text[:80] + \"...\" if len(chunk.text) > 80 else chunk.text\n",
    "        \n",
    "        print(f\"\\nPage {chunk.page_number}, Item {chunk.item_number}{overlap_flag}\")\n",
    "        print(f\"  \\\"{text_preview}\\\"\")\n",
    "    \n",
    "    # Page distribution\n",
    "    from collections import defaultdict\n",
    "    chunks_per_page = defaultdict(int)\n",
    "    for chunk in paragraph_doc_chunks:\n",
    "        chunks_per_page[chunk.page_number] += 1\n",
    "    \n",
    "    print(f\"\\nüìä Distribution by Page:\")\n",
    "    for page in sorted(chunks_per_page.keys())[:5]:  # First 5 pages\n",
    "        print(f\"   Page {page}: {chunks_per_page[page]} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 6b: Creating DocumentChunk Objects (Sentence Mode)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'docling_doc' not in locals():\n",
    "    print(\"\\n‚ùå ERROR: No DoclingDocument available\")\n",
    "else:\n",
    "    # Chunk in sentence mode\n",
    "    sentence_doc_chunks = chunker.chunk_document(\n",
    "        docling_doc, \n",
    "        mode=ChunkingMode.SENTENCE\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Document chunks created\")\n",
    "    print(f\"   Total chunks: {len(sentence_doc_chunks)}\")\n",
    "    print(f\"   Mode: {ChunkingMode.SENTENCE.value}\")\n",
    "    \n",
    "    # Show first 15 chunks with hierarchical numbering\n",
    "    print(f\"\\nüìã Document Chunks with Hierarchical Numbering (first 15):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for chunk in sentence_doc_chunks[:15]:\n",
    "        overlap_flag = \" [OVERLAP]\" if chunk.is_overlap else \"\"\n",
    "        \n",
    "        print(f\"\\nPage {chunk.page_number}, Item {chunk.item_number}{overlap_flag}\")\n",
    "        print(f\"  \\\"{chunk.text}\\\"\")\n",
    "    \n",
    "    # Analyze hierarchical structure\n",
    "    print(f\"\\nüìä Hierarchical Structure Analysis:\")\n",
    "    \n",
    "    # Count base chunks (items like 1.x, 2.x, 3.x)\n",
    "    base_items = set()\n",
    "    for chunk in sentence_doc_chunks:\n",
    "        if '.' in chunk.item_number:\n",
    "            base_items.add(chunk.item_number.split('.')[0])\n",
    "    \n",
    "    print(f\"   Total sentences: {len(sentence_doc_chunks)}\")\n",
    "    print(f\"   Total base chunks (paragraph-level): {len(base_items)}\")\n",
    "    print(f\"   Avg sentences per base chunk: {len(sentence_doc_chunks) / len(base_items):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparing Chunking Modes\n",
    "\n",
    "Let's compare the two chunking modes side-by-side to understand their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PART 7: Chunking Mode Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'paragraph_doc_chunks' in locals() and 'sentence_doc_chunks' in locals():\n",
    "    print(f\"\\n{'Metric':<40} {'Paragraph':<15} {'Sentence':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    print(f\"{'Total chunks':<40} {len(paragraph_doc_chunks):<15} {len(sentence_doc_chunks):<15}\")\n",
    "    \n",
    "    # Average text length\n",
    "    para_avg_len = sum(len(c.text) for c in paragraph_doc_chunks) / len(paragraph_doc_chunks)\n",
    "    sent_avg_len = sum(len(c.text) for c in sentence_doc_chunks) / len(sentence_doc_chunks)\n",
    "    print(f\"{'Avg chunk length (chars)':<40} {para_avg_len:<15.0f} {sent_avg_len:<15.0f}\")\n",
    "    \n",
    "    # Chunks per page (average)\n",
    "    para_pages = set(c.page_number for c in paragraph_doc_chunks)\n",
    "    sent_pages = set(c.page_number for c in sentence_doc_chunks)\n",
    "    para_per_page = len(paragraph_doc_chunks) / len(para_pages)\n",
    "    sent_per_page = len(sentence_doc_chunks) / len(sent_pages)\n",
    "    print(f\"{'Avg chunks per page':<40} {para_per_page:<15.1f} {sent_per_page:<15.1f}\")\n",
    "    \n",
    "    # Overlap counts\n",
    "    para_overlap = sum(1 for c in paragraph_doc_chunks if c.is_overlap)\n",
    "    sent_overlap = sum(1 for c in sentence_doc_chunks if c.is_overlap)\n",
    "    print(f\"{'Chunks with overlap flag':<40} {para_overlap:<15} {sent_overlap:<15}\")\n",
    "    \n",
    "    print(f\"\\nüí° Recommendations:\")\n",
    "    print(f\"   ‚Ä¢ Use PARAGRAPH mode for: General document verification, faster processing\")\n",
    "    print(f\"   ‚Ä¢ Use SENTENCE mode for: Fine-grained verification, detailed fact-checking\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå Please run Part 6 and 6b first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Exporting Verification Shell\n",
    "\n",
    "Now that we have DocumentChunk objects, let's see how they would be exported for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PART 8: Exporting DocumentChunks to JSON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'paragraph_doc_chunks' in locals():\n",
    "    # Convert to JSON-serializable format\n",
    "    chunks_data = [\n",
    "        {\n",
    "            \"page_number\": chunk.page_number,\n",
    "            \"item_number\": chunk.item_number,\n",
    "            \"text\": chunk.text,\n",
    "            \"is_overlap\": chunk.is_overlap,\n",
    "            \"verified\": None,  # To be filled by AI\n",
    "            \"verification_score\": None,\n",
    "            \"verification_source\": \"\",\n",
    "            \"verification_note\": \"\"\n",
    "        }\n",
    "        for chunk in paragraph_doc_chunks[:5]  # First 5 for demo\n",
    "    ]\n",
    "    \n",
    "    # Pretty print JSON\n",
    "    print(\"\\nüìÑ Sample Export (first 5 chunks):\")\n",
    "    print(json.dumps(chunks_data, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = \"verification_shell.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(\n",
    "            [chunk.model_dump() for chunk in paragraph_doc_chunks], \n",
    "            f, \n",
    "            indent=2, \n",
    "            ensure_ascii=False\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n‚úì Exported {len(paragraph_doc_chunks)} chunks to: {output_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå No chunks available for export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete document processing pipeline:\n",
    "\n",
    "1. ‚úÖ **PDF Conversion** - Docling converts PDF to structured DoclingDocument\n",
    "2. ‚úÖ **DOCX Conversion** - LibreOffice ‚Üí PDF ‚Üí DoclingDocument for accurate pagination\n",
    "3. ‚úÖ **Hierarchical Chunking** - HybridChunker preserves document structure\n",
    "4. ‚úÖ **Paragraph Splitting** - RecursiveCharacterTextSplitter for paragraph-level chunks\n",
    "5. ‚úÖ **Sentence Splitting** - SpaCy sentencizer for fine-grained chunks\n",
    "6. ‚úÖ **DocumentChunk Creation** - Structured objects with page/item metadata\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Docling** provides robust PDF/DOCX parsing with metadata preservation\n",
    "- **HybridChunker** maintains document hierarchy while chunking\n",
    "- **Paragraph mode** creates ~100-char chunks with simple numbering (1, 2, 3...)\n",
    "- **Sentence mode** creates true sentence-level chunks with hierarchical numbering (1.1, 1.2...)\n",
    "- **DocumentChunk objects** are ready for AI verification with Gemini\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore `gemini_features.ipynb` to see AI verification in action\n",
    "- Run the full application: `./start_all.sh`\n",
    "- Check `backend/app/processing/` for implementation details\n",
    "- Try different documents and chunking modes to understand behavior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
