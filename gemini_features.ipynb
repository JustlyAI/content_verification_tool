{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Document Verification Demo\n",
    "\n",
    "This notebook demonstrates the core Gemini AI functionalities used in the Content Verification Tool:\n",
    "\n",
    "1. **Creating a Corpus** (File Search Store)\n",
    "2. **Uploading Reference Documents** with AI-generated metadata\n",
    "3. **Querying the Corpus** for information\n",
    "4. **Verification Layer** - verifying document chunks against the corpus\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install google-genai python-dotenv\n",
    "```\n",
    "\n",
    "## API Key Setup (choose one method)\n",
    "\n",
    "**Option 1: .env file** (recommended)\n",
    "```bash\n",
    "echo \"GEMINI_API_KEY=your_api_key_here\" >> .env\n",
    "```\n",
    "\n",
    "**Option 2: Environment variable**\n",
    "```bash\n",
    "export GEMINI_API_KEY=\"your_api_key_here\"\n",
    "```\n",
    "\n",
    "**Option 3: Set directly in notebook** (see next cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Gemini client initialized successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Load environment variables from .env file (if it exists)\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from .env, environment variable, or set manually below\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# OPTION 3: Set API key manually (uncomment and replace with your key)\n",
    "# api_key = \"your_api_key_here\"\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"GEMINI_API_KEY not found. Please set it using one of these methods:\\n\"\n",
    "        \"  1. Create a .env file with: GEMINI_API_KEY=your_key\\n\"\n",
    "        \"  2. Set environment variable: export GEMINI_API_KEY=your_key\\n\"\n",
    "        \"  3. Uncomment the line above and set api_key directly\"\n",
    "    )\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=api_key)\n",
    "print(\"‚úì Gemini client initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating a Corpus (File Search Store)\n",
    "\n",
    "A **File Search Store** acts as a corpus - a searchable knowledge base of reference documents.\n",
    "\n",
    "Key features:\n",
    "- Automatically chunks and indexes documents\n",
    "- Generates embeddings for semantic search\n",
    "- Supports metadata filtering\n",
    "- **Free** storage and query-time embedding (only pay for initial indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating File Search store: Demo Corpus - 1763268225\n",
      "\n",
      "‚úì Store created successfully!\n",
      "  Store ID: fileSearchStores/demo-corpus-1763268225-icl6yo5kvmtt\n",
      "  Display Name: Demo Corpus - 1763268225\n"
     ]
    }
   ],
   "source": [
    "# Create a File Search store (corpus)\n",
    "store_name_display = f\"Demo Corpus - {int(time.time())}\"\n",
    "\n",
    "print(f\"Creating File Search store: {store_name_display}\")\n",
    "store = client.file_search_stores.create(\n",
    "    config={'display_name': store_name_display}\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Store created successfully!\")\n",
    "print(f\"  Store ID: {store.name}\")\n",
    "print(f\"  Display Name: {store.display_name}\")\n",
    "\n",
    "# Save store name for later use\n",
    "CORPUS_STORE_NAME = store.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Uploading Documents with AI Metadata\n",
    "\n",
    "We'll:\n",
    "1. Create a sample reference document\n",
    "2. Use **Gemini Flash Lite** to generate metadata (summary, keywords, document type)\n",
    "3. Upload to the File Search store with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Sample reference document created\n",
      "  Content preview: SERVICE AGREEMENT\n",
      "\n",
      "This Service Agreement (\"Agreement\") is entered into on January 15, 2024, between...\n"
     ]
    }
   ],
   "source": [
    "# Create a sample reference document\n",
    "sample_doc_content = \"\"\"SERVICE AGREEMENT\n",
    "\n",
    "This Service Agreement (\"Agreement\") is entered into on January 15, 2024, between:\n",
    "\n",
    "Company A (\"Provider\") - 123 Main Street, San Francisco, CA 94102\n",
    "Company B (\"Client\") - 456 Market Street, San Francisco, CA 94103\n",
    "\n",
    "TERMS AND CONDITIONS:\n",
    "\n",
    "1. Services: Provider agrees to deliver software development services as specified in the Statement of Work.\n",
    "\n",
    "2. Payment Terms: Client agrees to pay $150,000 for the services, payable in three installments:\n",
    "   - $50,000 upon signing\n",
    "   - $50,000 at project midpoint\n",
    "   - $50,000 upon completion\n",
    "\n",
    "3. Timeline: The project shall commence on February 1, 2024, and be completed by June 30, 2024.\n",
    "\n",
    "4. Intellectual Property: All work product shall become the property of the Client upon final payment.\n",
    "\n",
    "5. Confidentiality: Both parties agree to maintain confidentiality of proprietary information.\n",
    "\n",
    "Signed:\n",
    "John Smith, CEO, Company A\n",
    "Jane Doe, CEO, Company B\n",
    "\"\"\"\n",
    "\n",
    "# Save to temporary file\n",
    "import tempfile\n",
    "temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')\n",
    "temp_file.write(sample_doc_content)\n",
    "temp_file.close()\n",
    "\n",
    "print(\"‚úì Sample reference document created\")\n",
    "print(f\"  Content preview: {sample_doc_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Generate Metadata with Gemini Flash Lite\n",
    "\n",
    "We use **gemini-2.5-flash-lite** for fast, cost-effective metadata generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to Gemini for metadata extraction...\n",
      "‚úì File processed successfully: FileState.ACTIVE\n",
      "Requesting structured metadata response...\n",
      "Raw response length: 907 characters\n",
      "Parsed attribute available: True\n",
      "‚úì Metadata parsed, validated, and serialized to dict\n",
      "  Document Type: Service Agreement\n",
      "  Summary Preview: This Service Agreement outlines the terms for software development services between Company A (Provi...\n",
      "  Keywords: Service Agreement, Software Development, Company A, Company B, Payment Terms\n",
      "Cleaning up uploaded file from Gemini File API...\n",
      "Metadata dictionary ready for downstream cells.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "GEMINI_METADATA_MODEL = \"gemini-2.5-flash-lite\"\n",
    "CASE_CONTEXT = (\n",
    "    \"This is a contract verification case for software development services between two companies.\"\n",
    ")\n",
    "METADATA_PROMPT = f\"\"\"Analyze this document in the context of: {CASE_CONTEXT}\n",
    "\n",
    "Provide a JSON response with the following fields:\n",
    "- summary: A 2-3 sentence summary of the document\n",
    "- contextualization: How this document relates to the case context\n",
    "- document_type: The type of document (e.g., contract, invoice, receipt, report)\n",
    "- keywords: A list of 5-10 key terms or concepts from the document\n",
    "\n",
    "Return only valid JSON, no markdown formatting.\"\"\"\n",
    "\n",
    "class DocumentMetadata(BaseModel):\n",
    "    summary: str = Field(description=\"2-3 sentence summary of the document\")\n",
    "    contextualization: str = Field(description=\"Relationship to the provided case context\")\n",
    "    document_type: str = Field(description=\"Document classification (e.g., contract, invoice)\")\n",
    "    keywords: list[str] = Field(description=\"Key concepts extracted from the document\")\n",
    "\n",
    "metadata: dict[str, str | list[str]] | None = None\n",
    "uploaded_file = None\n",
    "\n",
    "try:\n",
    "    print(\"Uploading file to Gemini for metadata extraction...\")\n",
    "    uploaded_file = client.files.upload(file=temp_file.name)\n",
    "\n",
    "    while uploaded_file.state == \"PROCESSING\":\n",
    "        print(\"  Processing upload...\")\n",
    "        time.sleep(1)\n",
    "        uploaded_file = client.files.get(name=uploaded_file.name)\n",
    "\n",
    "    print(f\"‚úì File processed successfully: {uploaded_file.state}\")\n",
    "\n",
    "    print(\"Requesting structured metadata response...\")\n",
    "    response_best = client.models.generate_content(\n",
    "        model=GEMINI_METADATA_MODEL,\n",
    "        contents=[uploaded_file, METADATA_PROMPT],\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=DocumentMetadata,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(f\"Raw response length: {len(response_best.text)} characters\")\n",
    "    has_parsed = hasattr(response_best, \"parsed\") and response_best.parsed is not None\n",
    "    print(f\"Parsed attribute available: {has_parsed}\")\n",
    "\n",
    "    metadata_model = (\n",
    "        response_best.parsed\n",
    "        if has_parsed\n",
    "        else DocumentMetadata(**json.loads(response_best.text))\n",
    "    )\n",
    "    metadata = metadata_model.model_dump()\n",
    "\n",
    "    print(\"‚úì Metadata parsed, validated, and serialized to dict\")\n",
    "    print(f\"  Document Type: {metadata['document_type']}\")\n",
    "    summary_preview = (\n",
    "        metadata[\"summary\"][:100] + \"...\"\n",
    "        if len(metadata[\"summary\"]) > 100\n",
    "        else metadata[\"summary\"]\n",
    "    )\n",
    "    keywords_preview = \", \".join(metadata[\"keywords\"][:5]) if metadata[\"keywords\"] else \"n/a\"\n",
    "    print(f\"  Summary Preview: {summary_preview}\")\n",
    "    print(f\"  Keywords: {keywords_preview}\")\n",
    "finally:\n",
    "    if uploaded_file is not None:\n",
    "        print(\"Cleaning up uploaded file from Gemini File API...\")\n",
    "        client.files.delete(name=uploaded_file.name)\n",
    "\n",
    "print(\"Metadata dictionary ready for downstream cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Upload to File Search Store with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding file to File Search store...\n",
      "Upload operation started: fileSearchStores/demo-corpus-1763268225-icl6yo5kvmtt/upload/operations/service-agreement-company-a-8hnxmbbox10g\n",
      "Indexing document (this may take a few seconds)...\n",
      "\n",
      "‚úì Document upload initiated!\n",
      "  Store: fileSearchStores/demo-corpus-1763268225-icl6yo5kvmtt\n",
      "  Operation: fileSearchStores/demo-corpus-1763268225-icl6yo5kvmtt/upload/operations/service-agreement-company-a-8hnxmbbox10g\n",
      "\n",
      "Note: The file is being indexed in the background.\n",
      "It should be available for queries within 30-60 seconds.\n",
      "\n",
      "‚úì Temporary file cleaned up\n"
     ]
    }
   ],
   "source": [
    "# Create custom metadata for File Search\n",
    "custom_metadata = [\n",
    "    types.CustomMetadata(\n",
    "        key=\"summary\",\n",
    "        string_value=metadata['summary'][:256]  # Limit to 256 chars (API max)\n",
    "    ),\n",
    "    types.CustomMetadata(\n",
    "        key=\"document_type\",\n",
    "        string_value=metadata['document_type']\n",
    "    ),\n",
    "    types.CustomMetadata(\n",
    "        key=\"keywords\",\n",
    "        string_list_value=types.StringList(values=metadata['keywords'][:10])\n",
    "    )\n",
    "]\n",
    "\n",
    "# Upload file directly to File Search store\n",
    "# Note: This SDK version expects a file path, not a File object\n",
    "print(\"\\nAdding file to File Search store...\")\n",
    "operation = client.file_search_stores.upload_to_file_search_store(\n",
    "    file_search_store_name=CORPUS_STORE_NAME,\n",
    "    file=temp_file.name,  # Pass the file path\n",
    "    config=types.UploadToFileSearchStoreConfig(\n",
    "        custom_metadata=custom_metadata,\n",
    "        display_name='Service Agreement - Company A & B'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Upload operation started: {operation.name}\")\n",
    "\n",
    "# Wait for indexing (simplified - just wait without polling)\n",
    "print(\"Indexing document (this may take a few seconds)...\")\n",
    "time.sleep(10)  # Give it time to index\n",
    "\n",
    "print(\"\\n‚úì Document upload initiated!\")\n",
    "print(f\"  Store: {CORPUS_STORE_NAME}\")\n",
    "print(f\"  Operation: {operation.name}\")\n",
    "print(\"\\nNote: The file is being indexed in the background.\")\n",
    "print(\"It should be available for queries within 30-60 seconds.\")\n",
    "\n",
    "# Clean up temp file\n",
    "Path(temp_file.name).unlink()\n",
    "print(\"\\n‚úì Temporary file cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Querying the Corpus\n",
    "\n",
    "Now that we have a corpus with indexed documents, let's query it using Gemini's File Search capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUERYING THE CORPUS\n",
      "================================================================================\n",
      "\n",
      "üìù Query 1: What is the payment amount mentioned in the contract?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üí° Answer: The contract, a Service Agreement between Company A and Company B, states that the client agrees to pay $150,000 for the services. This payment is to be made in three installments of $50,000 each: one upon signing, one at the project midpoint, and the final one upon completion.\n",
      "\n",
      "üìö Sources (1 citations):\n",
      "\n",
      "üìù Query 2: When does the project start and end?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üí° Answer: The project is scheduled to begin on February 1, 2024, and is expected to conclude by June 30, 2024.\n",
      "\n",
      "üìö Sources (1 citations):\n",
      "\n",
      "üìù Query 3: Who are the parties involved in this agreement?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üí° Answer: The parties involved in this agreement are Company A, which is referred to as the \"Provider\", and Company B, which is referred to as the \"Client\".\n",
      "\n",
      "üìö Sources (1 citations):\n",
      "\n",
      "üìù Query 4: What are the payment terms?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üí° Answer: The client has agreed to pay a total of $150,000 for the services, which will be disbursed in three separate installments. The first payment of $50,000 is due upon the signing of the agreement. A second installment of $50,000 is to be paid at the midpoint of the project. The final payment of $50,000 will be made upon the completion of the project.\n",
      "\n",
      "üìö Sources (1 citations):\n"
     ]
    }
   ],
   "source": [
    "# Define queries to test\n",
    "queries = [\n",
    "    \"What is the payment amount mentioned in the contract?\",\n",
    "    \"When does the project start and end?\",\n",
    "    \"Who are the parties involved in this agreement?\",\n",
    "    \"What are the payment terms?\"\n",
    "]\n",
    "\n",
    "# Configure File Search tool\n",
    "file_search_tool = types.Tool(\n",
    "    file_search=types.FileSearch(\n",
    "        file_search_store_names=[CORPUS_STORE_NAME]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUERYING THE CORPUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\nüìù Query {i}: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Query with File Search\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=query,\n",
    "        config=types.GenerateContentConfig(\n",
    "            tools=[file_search_tool]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüí° Answer: {response.text}\\n\")\n",
    "    \n",
    "    # Show grounding sources if available\n",
    "    if hasattr(response, 'candidates') and response.candidates:\n",
    "        candidate = response.candidates[0]\n",
    "        if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:\n",
    "            if hasattr(candidate.grounding_metadata, 'grounding_chunks'):\n",
    "                chunks = candidate.grounding_metadata.grounding_chunks\n",
    "                if chunks:\n",
    "                    print(f\"üìö Sources ({len(chunks)} citations):\")\n",
    "                    for j, chunk in enumerate(chunks[:3], 1):  # Show first 3\n",
    "                        if hasattr(chunk, 'document') and chunk.document:\n",
    "                            title = getattr(chunk.document, 'title', 'Document')\n",
    "                            print(f\"  {j}. {title}\")\n",
    "                        if hasattr(chunk, 'content') and hasattr(chunk.content, 'text'):\n",
    "                            excerpt = chunk.content.text[:150]\n",
    "                            print(f\"     \\\"{excerpt}...\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Verification Layer\n",
    "\n",
    "The **verification layer** checks whether specific claims/chunks from a target document are supported by the corpus.\n",
    "\n",
    "This is the core functionality of the Document Verification Assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Define Chunks to Verify\n",
    "\n",
    "These are statements from a document we want to verify against our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Defined 5 chunks for verification\n",
      "  ‚Ä¢ Page 1, Item 1: The contract was signed on January 15, 2024....\n",
      "  ‚Ä¢ Page 1, Item 2: Client agrees to pay $150,000 for the services....\n",
      "  ‚Ä¢ Page 1, Item 3: The project timeline is from February 1 to June 30, 2024....\n",
      "  ‚Ä¢ Page 1, Item 4: The parties agree to a 90-day warranty period....\n",
      "  ‚Ä¢ Page 1, Item 5: All work product becomes the property of the Client upon fin...\n"
     ]
    }
   ],
   "source": [
    "# Define chunks to verify\n",
    "chunks_to_verify = [\n",
    "    {\n",
    "        \"page\": 1,\n",
    "        \"item\": \"1\",\n",
    "        \"text\": \"The contract was signed on January 15, 2024.\"\n",
    "    },\n",
    "    {\n",
    "        \"page\": 1,\n",
    "        \"item\": \"2\",\n",
    "        \"text\": \"Client agrees to pay $150,000 for the services.\"\n",
    "    },\n",
    "    {\n",
    "        \"page\": 1,\n",
    "        \"item\": \"3\",\n",
    "        \"text\": \"The project timeline is from February 1 to June 30, 2024.\"\n",
    "    },\n",
    "    {\n",
    "        \"page\": 1,\n",
    "        \"item\": \"4\",\n",
    "        \"text\": \"The parties agree to a 90-day warranty period.\"  # FALSE - not in document\n",
    "    },\n",
    "    {\n",
    "        \"page\": 1,\n",
    "        \"item\": \"5\",\n",
    "        \"text\": \"All work product becomes the property of the Client upon final payment.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìã Defined {len(chunks_to_verify)} chunks for verification\")\n",
    "for chunk in chunks_to_verify:\n",
    "    print(f\"  ‚Ä¢ Page {chunk['page']}, Item {chunk['item']}: {chunk['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Verify Each Chunk Against Corpus\n",
    "\n",
    "We use **Gemini 2.5 Flash** with File Search to:\n",
    "1. Check if the content is supported by the corpus\n",
    "2. Assign a confidence score (1-10)\n",
    "3. Provide source citations\n",
    "4. Explain the reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFICATION LAYER - Verifying Chunks Against Corpus (FIXED)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üìÑ Page 1, Item 1\n",
      "üìù Chunk: \"The contract was signed on January 15, 2024.\"\n",
      "================================================================================\n",
      "\n",
      "‚úÖ VERIFIED: True\n",
      "üìä Confidence Score: 10/10\n",
      "üìö Source: cite: 1\n",
      "üí≠ Note: The Service Agreement explicitly states that it was entered into on January 15, 2024.\n",
      "\n",
      "üîó Grounding Citations (1):\n",
      "  1. Service Agreement - Company A & B\n",
      "     \"SERVICE AGREEMENT\n",
      "\n",
      "This Service Agreement (\"Agreement\") is entered into on January 15, 2024, between:\n",
      "\n",
      "Company A (\"Provider\") - 123 Main Street, San F...\"\n",
      "\n",
      "================================================================================\n",
      "üìÑ Page 1, Item 2\n",
      "üìù Chunk: \"Client agrees to pay $150,000 for the services.\"\n",
      "================================================================================\n",
      "\n",
      "‚úÖ VERIFIED: True\n",
      "üìä Confidence Score: 10/10\n",
      "üìö Source: cite: 1\n",
      "üí≠ Note: The Service Agreement explicitly states that the Client agrees to pay $150,000 for the services.\n",
      "\n",
      "üîó Grounding Citations (1):\n",
      "  1. Service Agreement - Company A & B\n",
      "     \"SERVICE AGREEMENT\n",
      "\n",
      "This Service Agreement (\"Agreement\") is entered into on January 15, 2024, between:\n",
      "\n",
      "Company A (\"Provider\") - 123 Main Street, San F...\"\n",
      "\n",
      "================================================================================\n",
      "üìÑ Page 1, Item 3\n",
      "üìù Chunk: \"The project timeline is from February 1 to June 30, 2024.\"\n",
      "================================================================================\n",
      "\n",
      "‚úÖ VERIFIED: True\n",
      "üìä Confidence Score: 10/10\n",
      "üìö Source: cite: 1\n",
      "üí≠ Note: The Service Agreement explicitly states that the project shall commence on February 1, 2024, and be completed by June 30, 2024.\n",
      "\n",
      "üîó Grounding Citations (1):\n",
      "  1. Service Agreement - Company A & B\n",
      "     \"SERVICE AGREEMENT\n",
      "\n",
      "This Service Agreement (\"Agreement\") is entered into on January 15, 2024, between:\n",
      "\n",
      "Company A (\"Provider\") - 123 Main Street, San F...\"\n",
      "\n",
      "================================================================================\n",
      "üìÑ Page 1, Item 4\n",
      "üìù Chunk: \"The parties agree to a 90-day warranty period.\"\n",
      "================================================================================\n",
      "\n",
      "‚ùå VERIFIED: False\n",
      "üìä Confidence Score: 2/10\n",
      "üìö Source: No match found\n",
      "üí≠ Note: The provided Service Agreement between Company A and Company B does not contain any information regarding a warranty period, including a 90-day warranty period.\n",
      "\n",
      "üîó Grounding Citations (1):\n",
      "  1. Service Agreement - Company A & B\n",
      "     \"SERVICE AGREEMENT\n",
      "\n",
      "This Service Agreement (\"Agreement\") is entered into on January 15, 2024, between:\n",
      "\n",
      "Company A (\"Provider\") - 123 Main Street, San F...\"\n",
      "\n",
      "================================================================================\n",
      "üìÑ Page 1, Item 5\n",
      "üìù Chunk: \"All work product becomes the property of the Client upon final payment.\"\n",
      "================================================================================\n",
      "\n",
      "‚úÖ VERIFIED: True\n",
      "üìä Confidence Score: 10/10\n",
      "üìö Source: cite: 1\n",
      "üí≠ Note: The Service Agreement explicitly states that all work product becomes the property of the Client upon final payment under the 'Intellectual Property' section.\n",
      "\n",
      "üîó Grounding Citations (1):\n",
      "  1. Service Agreement - Company A & B\n",
      "     \"SERVICE AGREEMENT\n",
      "\n",
      "This Service Agreement (\"Agreement\") is entered into on January 15, 2024, between:\n",
      "\n",
      "Company A (\"Provider\") - 123 Main Street, San F...\"\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìä Summary:\n",
      "  Total Chunks: 5\n",
      "  Verified: 4\n",
      "  Total Grounding Citations: 5\n",
      "\n",
      "‚úÖ SUCCESS: File Search is working (5 citations)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FIXED VERIFICATION CODE - Replace cell-19 in gemini_demo.ipynb with this\n",
    "\n",
    "ROOT CAUSE IDENTIFIED:\n",
    "Using `response_schema` (Pydantic) with tools (File Search) causes the model\n",
    "to BYPASS the File Search tool entirely! This is why grounding citations are\n",
    "never returned.\n",
    "\n",
    "SOLUTION:\n",
    "- Remove `response_schema` parameter\n",
    "- Use `response_mime_type=\"application/json\"` ONLY\n",
    "- Manually parse and validate the JSON response\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "# Define Pydantic schema for validation (NOT for API)\n",
    "class VerificationResult(BaseModel):\n",
    "    verified: bool = Field(description=\"True if content is found/supported in reference docs\")\n",
    "    verification_score: int = Field(description=\"Confidence level 1-10\", ge=1, le=10)\n",
    "    verification_source: str = Field(description=\"Citation with document name and location\")\n",
    "    verification_note: str = Field(description=\"Brief explanation of reasoning\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION LAYER - Verifying Chunks Against Corpus (FIXED)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "verification_results = []\n",
    "\n",
    "for chunk in chunks_to_verify:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìÑ Page {chunk['page']}, Item {chunk['item']}\")\n",
    "    print(f\"üìù Chunk: \\\"{chunk['text']}\\\"\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Build verification prompt with explicit JSON schema\n",
    "    case_context = \"\"\"Service Agreement between Company A and Company B.\"\"\"\n",
    "\n",
    "    verification_prompt = f\"\"\"You are a document verification assistant with access to reference documents.\n",
    "\n",
    "## CONTEXT: \n",
    "\n",
    "{case_context}\n",
    "\n",
    "## TASK:\n",
    "\n",
    "Verify if the following statement is supported by the reference documents.\n",
    "\n",
    "## STATEMENT:\n",
    "\"{chunk['text']}\"\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Search the reference documents for information about this statement\n",
    "2. If you find supporting evidence, mark verified=true with high confidence (7-10)\n",
    "3. If you find contradicting evidence, mark verified=false and explain\n",
    "4. If you find no relevant information, mark verified=false with low confidence (1-3)\n",
    "\n",
    "REQUIRED JSON OUTPUT FORMAT:\n",
    "{{\n",
    "  \"verified\": boolean,\n",
    "  \"verification_score\": integer (1-10),\n",
    "  \"verification_source\": \"citation or 'No match found'\",\n",
    "  \"verification_note\": \"brief explanation\"\n",
    "}}\n",
    "\n",
    "Provide ONLY the JSON object, no other text.\"\"\"\n",
    "\n",
    "    # Verify with Gemini Flash + File Search\n",
    "    # CRITICAL: Do NOT use response_schema - it disables File Search!\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=verification_prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.1,\n",
    "            # response_mime_type=\"application/json\",  # Forces JSON format\n",
    "            # NO response_schema! This is what breaks File Search!\n",
    "            tools=[file_search_tool]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Parse and validate response\n",
    "    try:\n",
    "        # Clean response text (may have preamble or markdown)\n",
    "        response_text = response.text.strip()\n",
    "\n",
    "        # Remove common preambles\n",
    "        if response_text.startswith(\"Here is\") or response_text.startswith(\"```\"):\n",
    "            # Find JSON block\n",
    "            if \"```json\" in response_text:\n",
    "                start = response_text.find(\"```json\") + 7\n",
    "                end = response_text.find(\"```\", start)\n",
    "                response_text = response_text[start:end].strip()\n",
    "            elif response_text.startswith(\"```\"):\n",
    "                lines = response_text.split('\\n')\n",
    "                response_text = '\\n'.join(lines[1:-1])  # Remove first and last lines\n",
    "            else:\n",
    "                # Remove first line if it's preamble\n",
    "                lines = response_text.split('\\n')\n",
    "                response_text = '\\n'.join(lines[1:])\n",
    "\n",
    "        # Parse JSON\n",
    "        result_dict = json.loads(response_text)\n",
    "\n",
    "        # Validate with Pydantic\n",
    "        validated_result = VerificationResult(**result_dict)\n",
    "        result = {\n",
    "            \"verified\": validated_result.verified,\n",
    "            \"verification_score\": validated_result.verification_score,\n",
    "            \"verification_source\": validated_result.verification_source,\n",
    "            \"verification_note\": validated_result.verification_note\n",
    "        }\n",
    "\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Failed to parse/validate response\")\n",
    "        print(f\"Response text preview: {response.text[:300]}...\")\n",
    "        print(f\"Error: {e}\")\n",
    "        result = {\n",
    "            \"verified\": False,\n",
    "            \"verification_score\": 1,\n",
    "            \"verification_source\": \"Parse error\",\n",
    "            \"verification_note\": f\"Failed to parse: {str(e)}\"\n",
    "        }\n",
    "\n",
    "    # Extract grounding citations from API metadata\n",
    "    actual_citations = []\n",
    "    if hasattr(response, 'candidates') and response.candidates:\n",
    "        candidate = response.candidates[0]\n",
    "        if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:\n",
    "            if hasattr(candidate.grounding_metadata, 'grounding_chunks'):\n",
    "                for grounding_chunk in candidate.grounding_metadata.grounding_chunks:\n",
    "                    citation = {}\n",
    "\n",
    "                    # File Search uses retrieved_context\n",
    "                    if hasattr(grounding_chunk, 'retrieved_context'):\n",
    "                        ctx = grounding_chunk.retrieved_context\n",
    "                        citation[\"title\"] = getattr(ctx, 'title', 'Document')\n",
    "                        citation[\"excerpt\"] = getattr(ctx, 'text', '')[:300]\n",
    "                    # Fallback to document attribute\n",
    "                    elif hasattr(grounding_chunk, 'document') and grounding_chunk.document:\n",
    "                        citation[\"title\"] = getattr(grounding_chunk.document, 'title', 'Document')\n",
    "                        if hasattr(grounding_chunk, 'content') and hasattr(grounding_chunk.content, 'text'):\n",
    "                            citation[\"excerpt\"] = grounding_chunk.content.text[:300]\n",
    "\n",
    "                    if citation:\n",
    "                        actual_citations.append(citation)\n",
    "\n",
    "    # Add to results\n",
    "    result['chunk'] = chunk\n",
    "    result['actual_citations'] = actual_citations\n",
    "    verification_results.append(result)\n",
    "\n",
    "    # Display result\n",
    "    verified_icon = \"‚úÖ\" if result.get('verified', False) else \"‚ùå\"\n",
    "    print(f\"\\n{verified_icon} VERIFIED: {result.get('verified', False)}\")\n",
    "    print(f\"üìä Confidence Score: {result.get('verification_score', 0)}/10\")\n",
    "    print(f\"üìö Source: {result.get('verification_source', 'N/A')}\")\n",
    "    print(f\"üí≠ Note: {result.get('verification_note', 'N/A')}\")\n",
    "\n",
    "    if actual_citations:\n",
    "        print(f\"\\nüîó Grounding Citations ({len(actual_citations)}):\")\n",
    "        for i, citation in enumerate(actual_citations[:2], 1):  # Show first 2\n",
    "            print(f\"  {i}. {citation.get('title', 'Unknown')}\")\n",
    "            if 'excerpt' in citation:\n",
    "                excerpt = citation['excerpt'][:150]\n",
    "                print(f\"     \\\"{excerpt}...\\\"\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: No grounding citations - File Search may not have been used\")\n",
    "\n",
    "    # Small delay to avoid rate limits\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate and display statistics\n",
    "total_chunks = len(verification_results)\n",
    "verified_count = sum(1 for r in verification_results if r['verified'])\n",
    "total_citations = sum(len(r['actual_citations']) for r in verification_results)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  Total Chunks: {total_chunks}\")\n",
    "print(f\"  Verified: {verified_count}\")\n",
    "print(f\"  Total Grounding Citations: {total_citations}\")\n",
    "\n",
    "if total_citations == 0:\n",
    "    print(f\"\\n‚ùå PROBLEM: No grounding citations found!\")\n",
    "    print(f\"   This means File Search was not used.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ SUCCESS: File Search is working ({total_citations} citations)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä VERIFICATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total Chunks: 5\n",
      "‚úÖ Verified: 4 (80.0%)\n",
      "‚ùå Unverified: 1 (20.0%)\n",
      "üìä Average Confidence: 8.4/10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "BREAKDOWN BY CHUNK:\n",
      "--------------------------------------------------------------------------------\n",
      "‚úÖ Page 1, Item 1 - Score: 10/10\n",
      "   \"The contract was signed on January 15, 2024....\"\n",
      "\n",
      "‚úÖ Page 1, Item 2 - Score: 10/10\n",
      "   \"Client agrees to pay $150,000 for the services....\"\n",
      "\n",
      "‚úÖ Page 1, Item 3 - Score: 10/10\n",
      "   \"The project timeline is from February 1 to June 30, 2024....\"\n",
      "\n",
      "‚ùå Page 1, Item 4 - Score: 2/10\n",
      "   \"The parties agree to a 90-day warranty period....\"\n",
      "\n",
      "‚úÖ Page 1, Item 5 - Score: 10/10\n",
      "   \"All work product becomes the property of the Client upon final payment...\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "total_chunks = len(verification_results)\n",
    "verified_count = sum(1 for r in verification_results if r['verified'])\n",
    "unverified_count = total_chunks - verified_count\n",
    "avg_score = sum(r['verification_score'] for r in verification_results) / total_chunks\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal Chunks: {total_chunks}\")\n",
    "print(f\"‚úÖ Verified: {verified_count} ({verified_count/total_chunks*100:.1f}%)\")\n",
    "print(f\"‚ùå Unverified: {unverified_count} ({unverified_count/total_chunks*100:.1f}%)\")\n",
    "print(f\"üìä Average Confidence: {avg_score:.1f}/10\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"BREAKDOWN BY CHUNK:\")\n",
    "print(\"-\" * 80)\n",
    "for result in verification_results:\n",
    "    chunk = result['chunk']\n",
    "    icon = \"‚úÖ\" if result['verified'] else \"‚ùå\"\n",
    "    print(f\"{icon} Page {chunk['page']}, Item {chunk['item']} - Score: {result['verification_score']}/10\")\n",
    "    print(f\"   \\\"{chunk['text'][:70]}...\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Export Verification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Verification results exported to: verification_results.json\n"
     ]
    }
   ],
   "source": [
    "# Export to JSON\n",
    "output_data = {\n",
    "    \"corpus_store\": CORPUS_STORE_NAME,\n",
    "    \"case_context\": case_context,\n",
    "    \"total_chunks\": total_chunks,\n",
    "    \"verified_chunks\": verified_count,\n",
    "    \"average_confidence\": avg_score,\n",
    "    \"results\": verification_results\n",
    "}\n",
    "\n",
    "output_file = \"verification_results.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úì Verification results exported to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup: Delete File Search Store\n",
    "\n",
    "**Important:** File Search stores are free, but you may want to clean up test stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLEANUP - Deleting File Search Store\n",
      "================================================================================\n",
      "\n",
      "üìã Listing documents in store: fileSearchStores/demo-corpus-1763268225-icl6yo5kvmtt\n",
      "   ‚ö†Ô∏è  Could not list documents: 'FileSearchStores' object has no attribute 'list_documents'\n",
      "   Attempting to delete store with force flag...\n",
      "\n",
      "üóëÔ∏è  Deleting File Search store...\n",
      "‚úì Deleted File Search store: fileSearchStores/demo-corpus-1763268225-icl6yo5kvmtt\n",
      "\n",
      "================================================================================\n",
      "CLEANUP COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FIXED CLEANUP CELL - Properly delete File Search store\n",
    "\n",
    "ERROR: Cannot delete non-empty FileSearchStore\n",
    "SOLUTION: Delete all documents first, then delete the store\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANUP - Deleting File Search Store\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Step 1: List all documents in the store\n",
    "    print(f\"\\nüìã Listing documents in store: {CORPUS_STORE_NAME}\")\n",
    "\n",
    "    try:\n",
    "        documents = list(client.file_search_stores.list_documents(\n",
    "            file_search_store_name=CORPUS_STORE_NAME\n",
    "        ))\n",
    "        print(f\"   Found {len(documents)} document(s)\")\n",
    "\n",
    "        # Step 2: Delete each document\n",
    "        if documents:\n",
    "            print(f\"\\nüóëÔ∏è  Deleting documents...\")\n",
    "            for i, doc in enumerate(documents, 1):\n",
    "                doc_name = doc.name\n",
    "                print(f\"   {i}. Deleting: {doc_name}\")\n",
    "                try:\n",
    "                    client.file_search_stores.delete_document(name=doc_name)\n",
    "                    print(f\"      ‚úì Deleted\")\n",
    "                except Exception as e:\n",
    "                    print(f\"      ‚ö†Ô∏è  Error: {e}\")\n",
    "        else:\n",
    "            print(f\"   No documents to delete\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not list documents: {e}\")\n",
    "        print(f\"   Attempting to delete store with force flag...\")\n",
    "\n",
    "    # Step 3: Delete the store (with force flag to delete any remaining content)\n",
    "    print(f\"\\nüóëÔ∏è  Deleting File Search store...\")\n",
    "    client.file_search_stores.delete(\n",
    "        name=CORPUS_STORE_NAME,\n",
    "        config={'force': True}  # Force delete even if not empty\n",
    "    )\n",
    "    print(f\"‚úì Deleted File Search store: {CORPUS_STORE_NAME}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during cleanup: {e}\")\n",
    "    print(f\"\\nTo manually clean up, run:\")\n",
    "    print(f\"   client.file_search_stores.delete(name='{CORPUS_STORE_NAME}', config={{'force': True}})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANUP COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ‚úÖ **Creating a Corpus** - File Search Store for reference documents\n",
    "2. ‚úÖ **AI Metadata Generation** - Using Gemini Flash Lite to analyze and tag documents\n",
    "3. ‚úÖ **Querying the Corpus** - Semantic search with File Search\n",
    "4. ‚úÖ **Verification Layer** - Verifying chunks against corpus with citations\n",
    "\n",
    "### Key Models Used\n",
    "\n",
    "- **gemini-2.5-flash**: Fast, accurate model for verification and querying\n",
    "- **gemini-2.5-flash-lite**: Ultra-fast, cost-effective model for metadata generation\n",
    "- **File Search**: Fully managed RAG with automatic chunking and embeddings\n",
    "\n",
    "### Pricing\n",
    "\n",
    "- **File Search Storage**: Free\n",
    "- **File Search Querying**: Free\n",
    "- **Initial Indexing**: $0.15 per 1M tokens\n",
    "- **Flash Model**: $0.10 per 1M input tokens, $0.40 per 1M output tokens\n",
    "- **Flash Lite Model**: $0.075 per 1M input tokens, $0.30 per 1M output tokens\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore the full application in `backend/` and `frontend/`\n",
    "- Run tests in `tests/` to see comprehensive examples\n",
    "- Check `backend/app/verification/gemini_service.py` for production implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
