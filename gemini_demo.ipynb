{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Gemini Document Verification Demo\n\nThis notebook demonstrates the core Gemini AI functionalities used in the Document Verification Assistant:\n\n1. **Creating a Corpus** (File Search Store)\n2. **Uploading Reference Documents** with AI-generated metadata\n3. **Querying the Corpus** for information\n4. **Verification Layer** - verifying document chunks against the corpus\n\n## Prerequisites\n\n```bash\npip install google-genai python-dotenv\n```\n\n## API Key Setup (choose one method)\n\n**Option 1: .env file** (recommended)\n```bash\necho \"GEMINI_API_KEY=your_api_key_here\" >> .env\n```\n\n**Option 2: Environment variable**\n```bash\nexport GEMINI_API_KEY=\"your_api_key_here\"\n```\n\n**Option 3: Set directly in notebook** (see next cell)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Initialize Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport time\nimport json\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nfrom google import genai\nfrom google.genai import types\n\n# Load environment variables from .env file (if it exists)\nload_dotenv()\n\n# Get API key from .env, environment variable, or set manually below\napi_key = os.getenv(\"GEMINI_API_KEY\")\n\n# OPTION 3: Set API key manually (uncomment and replace with your key)\n# api_key = \"your_api_key_here\"\n\nif not api_key:\n    raise ValueError(\n        \"GEMINI_API_KEY not found. Please set it using one of these methods:\\n\"\n        \"  1. Create a .env file with: GEMINI_API_KEY=your_key\\n\"\n        \"  2. Set environment variable: export GEMINI_API_KEY=your_key\\n\"\n        \"  3. Uncomment the line above and set api_key directly\"\n    )\n\n# Initialize Gemini client\nclient = genai.Client(api_key=api_key)\nprint(\"âœ“ Gemini client initialized successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating a Corpus (File Search Store)\n",
    "\n",
    "A **File Search Store** acts as a corpus - a searchable knowledge base of reference documents.\n",
    "\n",
    "Key features:\n",
    "- Automatically chunks and indexes documents\n",
    "- Generates embeddings for semantic search\n",
    "- Supports metadata filtering\n",
    "- **Free** storage and query-time embedding (only pay for initial indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a File Search store (corpus)\n",
    "store_name_display = f\"Demo Corpus - {int(time.time())}\"\n",
    "\n",
    "print(f\"Creating File Search store: {store_name_display}\")\n",
    "store = client.file_search_stores.create(\n",
    "    config={'display_name': store_name_display}\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Store created successfully!\")\n",
    "print(f\"  Store ID: {store.name}\")\n",
    "print(f\"  Display Name: {store.display_name}\")\n",
    "\n",
    "# Save store name for later use\n",
    "CORPUS_STORE_NAME = store.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Uploading Documents with AI Metadata\n",
    "\n",
    "We'll:\n",
    "1. Create a sample reference document\n",
    "2. Use **Gemini Flash** to generate metadata (summary, keywords, document type)\n",
    "3. Upload to the File Search store with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample reference document\n",
    "sample_doc_content = \"\"\"SERVICE AGREEMENT\n",
    "\n",
    "This Service Agreement (\"Agreement\") is entered into on January 15, 2024, between:\n",
    "\n",
    "Company A (\"Provider\") - 123 Main Street, San Francisco, CA 94102\n",
    "Company B (\"Client\") - 456 Market Street, San Francisco, CA 94103\n",
    "\n",
    "TERMS AND CONDITIONS:\n",
    "\n",
    "1. Services: Provider agrees to deliver software development services as specified in the Statement of Work.\n",
    "\n",
    "2. Payment Terms: Client agrees to pay $150,000 for the services, payable in three installments:\n",
    "   - $50,000 upon signing\n",
    "   - $50,000 at project midpoint\n",
    "   - $50,000 upon completion\n",
    "\n",
    "3. Timeline: The project shall commence on February 1, 2024, and be completed by June 30, 2024.\n",
    "\n",
    "4. Intellectual Property: All work product shall become the property of the Client upon final payment.\n",
    "\n",
    "5. Confidentiality: Both parties agree to maintain confidentiality of proprietary information.\n",
    "\n",
    "Signed:\n",
    "John Smith, CEO, Company A\n",
    "Jane Doe, CEO, Company B\n",
    "\"\"\"\n",
    "\n",
    "# Save to temporary file\n",
    "import tempfile\n",
    "temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')\n",
    "temp_file.write(sample_doc_content)\n",
    "temp_file.close()\n",
    "\n",
    "print(\"âœ“ Sample reference document created\")\n",
    "print(f\"  Content preview: {sample_doc_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Generate Metadata with Gemini Flash\n",
    "\n",
    "We use **gemini-2.0-flash-exp** for fast, cost-effective metadata generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file to Gemini for analysis\n",
    "print(\"Uploading file to Gemini for metadata generation...\")\n",
    "uploaded_file = client.files.upload(file_path=temp_file.name)\n",
    "\n",
    "# Wait for processing\n",
    "while uploaded_file.state == \"PROCESSING\":\n",
    "    print(\"  Processing...\")\n",
    "    time.sleep(1)\n",
    "    uploaded_file = client.files.get(name=uploaded_file.name)\n",
    "\n",
    "print(f\"âœ“ File processed: {uploaded_file.state}\")\n",
    "\n",
    "# Generate metadata using Gemini Flash\n",
    "case_context = \"This is a contract verification case for software development services between two companies.\"\n",
    "\n",
    "metadata_prompt = f\"\"\"Analyze this document in the context of: {case_context}\n",
    "\n",
    "Provide a JSON response with the following fields:\n",
    "- summary: A 2-3 sentence summary of the document\n",
    "- contextualization: How this document relates to the case context\n",
    "- document_type: The type of document (e.g., contract, invoice, receipt, report)\n",
    "- keywords: A list of 5-10 key terms or concepts from the document\n",
    "\n",
    "Return only valid JSON, no markdown formatting.\"\"\"\n",
    "\n",
    "print(\"\\nGenerating metadata with Gemini Flash...\")\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[uploaded_file, metadata_prompt]\n",
    ")\n",
    "\n",
    "# Parse metadata\n",
    "response_text = response.text.strip()\n",
    "if response_text.startswith(\"```\"):\n",
    "    response_text = response_text.split(\"```\")[1]\n",
    "    if response_text.startswith(\"json\"):\n",
    "        response_text = response_text[4:]\n",
    "    response_text = response_text.strip()\n",
    "\n",
    "metadata = json.loads(response_text)\n",
    "\n",
    "print(\"\\nâœ“ Metadata generated:\")\n",
    "print(f\"  Document Type: {metadata['document_type']}\")\n",
    "print(f\"  Summary: {metadata['summary']}\")\n",
    "print(f\"  Keywords: {', '.join(metadata['keywords'][:5])}\")\n",
    "\n",
    "# Clean up the temporary analysis file\n",
    "client.files.delete(name=uploaded_file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Upload to File Search Store with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file to Gemini Files API\n",
    "print(\"Uploading file to Gemini Files API...\")\n",
    "file_for_store = client.files.upload(file_path=temp_file.name)\n",
    "\n",
    "# Wait for processing\n",
    "while file_for_store.state == \"PROCESSING\":\n",
    "    time.sleep(1)\n",
    "    file_for_store = client.files.get(name=file_for_store.name)\n",
    "\n",
    "print(f\"âœ“ File uploaded: {file_for_store.name}\")\n",
    "\n",
    "# Create custom metadata for File Search\n",
    "custom_metadata = [\n",
    "    types.CustomMetadata(\n",
    "        key=\"summary\",\n",
    "        string_value=metadata['summary'][:500]  # Limit to 500 chars\n",
    "    ),\n",
    "    types.CustomMetadata(\n",
    "        key=\"document_type\",\n",
    "        string_value=metadata['document_type']\n",
    "    ),\n",
    "    types.CustomMetadata(\n",
    "        key=\"keywords\",\n",
    "        string_list_value=types.StringList(values=metadata['keywords'][:10])\n",
    "    )\n",
    "]\n",
    "\n",
    "# Add file to File Search store\n",
    "print(\"\\nAdding file to File Search store...\")\n",
    "operation = client.file_search_stores.upload_to_file_search_store(\n",
    "    file_search_store_name=CORPUS_STORE_NAME,\n",
    "    file=file_for_store,\n",
    "    config={\n",
    "        'custom_metadata': custom_metadata,\n",
    "        'display_name': 'Service Agreement - Company A & B'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wait for indexing\n",
    "print(\"Indexing document (this may take a few seconds)...\")\n",
    "max_wait = 60\n",
    "elapsed = 0\n",
    "while not operation.done and elapsed < max_wait:\n",
    "    time.sleep(2)\n",
    "    elapsed += 2\n",
    "    operation = client.operations.get(name=operation.name)\n",
    "    print(f\"  Indexing... ({elapsed}s)\")\n",
    "\n",
    "if operation.done:\n",
    "    print(\"\\nâœ“ Document indexed successfully!\")\n",
    "    print(f\"  Store: {CORPUS_STORE_NAME}\")\n",
    "    print(f\"  File: {file_for_store.name}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Indexing still in progress (background processing)\")\n",
    "\n",
    "# Clean up temp file\n",
    "Path(temp_file.name).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Querying the Corpus\n",
    "\n",
    "Now that we have a corpus with indexed documents, let's query it using Gemini's File Search capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define queries to test\n",
    "queries = [\n",
    "    \"What is the payment amount mentioned in the contract?\",\n",
    "    \"When does the project start and end?\",\n",
    "    \"Who are the parties involved in this agreement?\",\n",
    "    \"What are the payment terms?\"\n",
    "]\n",
    "\n",
    "# Configure File Search tool\n",
    "file_search_tool = types.Tool(\n",
    "    file_search=types.FileSearch(\n",
    "        file_search_store_names=[CORPUS_STORE_NAME]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUERYING THE CORPUS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\nðŸ“ Query {i}: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Query with File Search\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        contents=query,\n",
    "        config=types.GenerateContentConfig(\n",
    "            tools=[file_search_tool]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Answer: {response.text}\\n\")\n",
    "    \n",
    "    # Show grounding sources if available\n",
    "    if hasattr(response, 'candidates') and response.candidates:\n",
    "        candidate = response.candidates[0]\n",
    "        if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:\n",
    "            if hasattr(candidate.grounding_metadata, 'grounding_chunks'):\n",
    "                chunks = candidate.grounding_metadata.grounding_chunks\n",
    "                if chunks:\n",
    "                    print(f\"ðŸ“š Sources ({len(chunks)} citations):\")\n",
    "                    for j, chunk in enumerate(chunks[:3], 1):  # Show first 3\n",
    "                        if hasattr(chunk, 'document') and chunk.document:\n",
    "                            title = getattr(chunk.document, 'title', 'Document')\n",
    "                            print(f\"  {j}. {title}\")\n",
    "                        if hasattr(chunk, 'content') and hasattr(chunk.content, 'text'):\n",
    "                            excerpt = chunk.content.text[:150]\n",
    "                            print(f\"     \\\"{excerpt}...\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Verification Layer\n",
    "\n",
    "The **verification layer** checks whether specific claims/chunks from a target document are supported by the corpus.\n",
    "\n",
    "This is the core functionality of the Document Verification Assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Define Chunks to Verify\n",
    "\n",
    "These are statements from a document we want to verify against our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunks to verify\n",
    "chunks_to_verify = [\n",
    "    {\n",
    "        \"page\": 1,\n",
    "        \"item\": \"1\",\n",
    "        \"text\": \"The contract was signed on January 15, 2024.\"\n",
    "    },\n",
    "    {\n",
    "        \"page\": 1,\n",
    "        \"item\": \"2\",\n",
    "        \"text\": \"Client agrees to pay $150,000 for the services.\"\n",
    "    },\n",
    "    {\n",
    "        \"page\": 1,\n",
    "        \"item\": \"3\",\n",
    "        \"text\": \"The project timeline is from February 1 to June 30, 2024.\"\n",
    "    },\n",
    "    {\n",
    "        \"page\": 1,\n",
    "        \"item\": \"4\",\n",
    "        \"text\": \"The parties agree to a 90-day warranty period.\"  # FALSE - not in document\n",
    "    },\n",
    "    {\n",
    "        \"page\": 1,\n",
    "        \"item\": \"5\",\n",
    "        \"text\": \"All work product becomes the property of the Client upon final payment.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“‹ Defined {len(chunks_to_verify)} chunks for verification\")\n",
    "for chunk in chunks_to_verify:\n",
    "    print(f\"  â€¢ Page {chunk['page']}, Item {chunk['item']}: {chunk['text'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Verify Each Chunk Against Corpus\n",
    "\n",
    "We use **Gemini Flash** with File Search to:\n",
    "1. Check if the content is supported by the corpus\n",
    "2. Assign a confidence score (1-10)\n",
    "3. Provide source citations\n",
    "4. Explain the reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION LAYER - Verifying Chunks Against Corpus\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "verification_results = []\n",
    "\n",
    "for chunk in chunks_to_verify:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ðŸ“„ Page {chunk['page']}, Item {chunk['item']}\")\n",
    "    print(f\"ðŸ“ Chunk: \\\"{chunk['text']}\\\"\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Build verification prompt\n",
    "    verification_prompt = f\"\"\"Context: {case_context}\n",
    "\n",
    "Verify this content against the reference documents:\n",
    "\n",
    "Page {chunk['page']}, Item {chunk['item']}:\n",
    "\"{chunk['text']}\"\n",
    "\n",
    "Please verify if this content appears in or is supported by the reference documents. Provide your response in JSON format with these fields:\n",
    "\n",
    "1. verified: (boolean) true if the content is found/supported, false otherwise\n",
    "2. confidence_score: (integer 1-10) how confident you are in this verification\n",
    "3. verification_source: (string) citation with document name and location\n",
    "4. verification_note: (string) brief explanation of your reasoning\n",
    "5. citations: (array) list of specific passages that support this verification, each with \"title\" and \"excerpt\" fields\n",
    "\n",
    "Return only valid JSON, no markdown formatting.\"\"\"\n",
    "    \n",
    "    # Verify with Gemini Flash + File Search\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        contents=verification_prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.1,  # Low temperature for consistent results\n",
    "            response_mime_type=\"application/json\",\n",
    "            tools=[file_search_tool]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Parse verification result\n",
    "    result = json.loads(response.text)\n",
    "    \n",
    "    # Extract grounding citations\n",
    "    actual_citations = []\n",
    "    if hasattr(response, 'candidates') and response.candidates:\n",
    "        candidate = response.candidates[0]\n",
    "        if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:\n",
    "            if hasattr(candidate.grounding_metadata, 'grounding_chunks'):\n",
    "                for grounding_chunk in candidate.grounding_metadata.grounding_chunks:\n",
    "                    citation = {}\n",
    "                    if hasattr(grounding_chunk, 'document') and grounding_chunk.document:\n",
    "                        citation[\"title\"] = getattr(grounding_chunk.document, 'title', 'Document')\n",
    "                    if hasattr(grounding_chunk, 'content') and hasattr(grounding_chunk.content, 'text'):\n",
    "                        citation[\"excerpt\"] = grounding_chunk.content.text\n",
    "                    if citation:\n",
    "                        actual_citations.append(citation)\n",
    "    \n",
    "    # Add to results\n",
    "    result['chunk'] = chunk\n",
    "    result['actual_citations'] = actual_citations\n",
    "    verification_results.append(result)\n",
    "    \n",
    "    # Display result\n",
    "    verified_icon = \"âœ…\" if result['verified'] else \"âŒ\"\n",
    "    print(f\"\\n{verified_icon} VERIFIED: {result['verified']}\")\n",
    "    print(f\"ðŸ“Š Confidence Score: {result['confidence_score']}/10\")\n",
    "    print(f\"ðŸ“š Source: {result['verification_source']}\")\n",
    "    print(f\"ðŸ’­ Note: {result['verification_note']}\")\n",
    "    \n",
    "    if actual_citations:\n",
    "        print(f\"\\nðŸ”— Grounding Citations ({len(actual_citations)}):\")\n",
    "        for i, citation in enumerate(actual_citations[:2], 1):  # Show first 2\n",
    "            print(f\"  {i}. {citation.get('title', 'Unknown')}\")\n",
    "            if 'excerpt' in citation:\n",
    "                excerpt = citation['excerpt'][:150]\n",
    "                print(f\"     \\\"{excerpt}...\\\"\")\n",
    "    \n",
    "    # Small delay to avoid rate limits\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: Verification Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "total_chunks = len(verification_results)\n",
    "verified_count = sum(1 for r in verification_results if r['verified'])\n",
    "unverified_count = total_chunks - verified_count\n",
    "avg_score = sum(r['confidence_score'] for r in verification_results) / total_chunks\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal Chunks: {total_chunks}\")\n",
    "print(f\"âœ… Verified: {verified_count} ({verified_count/total_chunks*100:.1f}%)\")\n",
    "print(f\"âŒ Unverified: {unverified_count} ({unverified_count/total_chunks*100:.1f}%)\")\n",
    "print(f\"ðŸ“Š Average Confidence: {avg_score:.1f}/10\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"BREAKDOWN BY CHUNK:\")\n",
    "print(\"-\" * 80)\n",
    "for result in verification_results:\n",
    "    chunk = result['chunk']\n",
    "    icon = \"âœ…\" if result['verified'] else \"âŒ\"\n",
    "    print(f\"{icon} Page {chunk['page']}, Item {chunk['item']} - Score: {result['confidence_score']}/10\")\n",
    "    print(f\"   \\\"{chunk['text'][:70]}...\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Export Verification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "output_data = {\n",
    "    \"corpus_store\": CORPUS_STORE_NAME,\n",
    "    \"case_context\": case_context,\n",
    "    \"total_chunks\": total_chunks,\n",
    "    \"verified_chunks\": verified_count,\n",
    "    \"average_confidence\": avg_score,\n",
    "    \"results\": verification_results\n",
    "}\n",
    "\n",
    "output_file = \"verification_results.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ“ Verification results exported to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup: Delete File Search Store\n",
    "\n",
    "**Important:** File Search stores are free, but you may want to clean up test stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the store\n",
    "# client.file_search_stores.delete(name=CORPUS_STORE_NAME)\n",
    "# print(f\"âœ“ Deleted File Search store: {CORPUS_STORE_NAME}\")\n",
    "\n",
    "print(\"\\nâš ï¸  Store not deleted. Uncomment code above to clean up.\")\n",
    "print(f\"   Store ID: {CORPUS_STORE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. âœ… **Creating a Corpus** - File Search Store for reference documents\n",
    "2. âœ… **AI Metadata Generation** - Using Gemini Flash to analyze and tag documents\n",
    "3. âœ… **Querying the Corpus** - Semantic search with File Search\n",
    "4. âœ… **Verification Layer** - Verifying chunks against corpus with citations\n",
    "\n",
    "### Key Models Used\n",
    "\n",
    "- **gemini-2.0-flash-exp**: Fast, cost-effective model for metadata and verification\n",
    "- **File Search**: Fully managed RAG with automatic chunking and embeddings\n",
    "\n",
    "### Pricing\n",
    "\n",
    "- **File Search Storage**: Free\n",
    "- **File Search Querying**: Free\n",
    "- **Initial Indexing**: $0.15 per 1M tokens\n",
    "- **Flash Model**: $0.10 per 1M input tokens, $0.40 per 1M output tokens\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore the full application in `backend/` and `frontend/`\n",
    "- Run tests in `tests/` to see comprehensive examples\n",
    "- Check `backend/app/gemini_service.py` for production implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}